{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccd1cca0-d3ae-4ca5-8cb8-444f0ce7849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "\n",
    "from transformers import (\n",
    "    ViTForImageClassification,\n",
    "    AutoImageProcessor,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38136ce4-5594-48dc-8933-fe170cc5557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeImageFolder(datasets.ImageFolder):\n",
    "    \"\"\"SafeImageFolder that ignores hidden classes like `.ipynb_checkpoints`.\"\"\"\n",
    "    def find_classes(self, directory: str):\n",
    "        classes, _ = super().find_classes(directory)\n",
    "        # drop any class directory that starts with a dot\n",
    "        classes = [c for c in classes if not c.startswith('.')]\n",
    "        classes = sorted(classes)\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d98a4cb-9ab9-4ca4-bd3d-767840b098c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "DATA_ROOT = \"/data/imagenet100-224/train\"  # class-subfolders inside here\n",
    "MODEL_NAME = \"google/vit-base-patch16-224\"\n",
    "OUTPUT_DIR = \"./vit-imagenet100-head-only\"\n",
    "VAL_SPLIT = 0.1\n",
    "SEED = 42\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32            # adjust to your GPU memory\n",
    "LR = 3e-4                  # a bit higher since we're training only the head\n",
    "WEIGHT_DECAY = 0.01\n",
    "WARMUP_RATIO = 0.05\n",
    "GRAD_ACCUM_STEPS = 1       # increase if you need effective larger batch\n",
    "AMP = True                 # mixed precision\n",
    "\n",
    "NUM_WORKERS = min(8, os.cpu_count() or 2)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db89d255-4cee-4748-beed-a22e858c6caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing dataset…\n",
      "Train: 117,000 | Val: 13,000 | Classes: 100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "535abe79b77347ddb5a4a94dac56a3ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Data\n",
    "# -----------------------------\n",
    "# We’ll use torchvision’s ImageFolder and apply the ViT processor in the collate_fn.\n",
    "print(\"Indexing dataset…\")\n",
    "full_ds = SafeImageFolder(DATA_ROOT)\n",
    "num_classes = len(full_ds.classes)\n",
    "\n",
    "# Build label maps from folder names\n",
    "label2id = {cls_name: i for cls_name, i in full_ds.class_to_idx.items()}\n",
    "id2label = {i: cls_name for cls_name, i in full_ds.class_to_idx.items()}\n",
    "\n",
    "# Train/val split from the single folder\n",
    "random.seed(SEED)\n",
    "N = len(full_ds)\n",
    "n_val = int(N * VAL_SPLIT)\n",
    "n_train = N - n_val\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=torch.Generator().manual_seed(SEED))\n",
    "print(f\"Train: {n_train:,} | Val: {n_val:,} | Classes: {num_classes}\")\n",
    "\n",
    "# Image processor handles resize, to-tensor, normalization for ViT\n",
    "processor = AutoImageProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: list of (PIL_image, label)\n",
    "    images, labels = zip(*batch)\n",
    "    enc = processor(images=list(images), return_tensors=\"pt\")\n",
    "    enc[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n",
    "    return enc\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea134b2d-4d7f-4368-9ea5-d71b3174032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([100]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([100, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: ['classifier.weight', 'classifier.bias']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1549/938967549.py:36: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
      "/tmp/ipykernel_1549/938967549.py:70: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=AMP):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Step 50/3657 | Loss 4.7320 | LR 0.000008\n",
      "Epoch 1 | Step 100/3657 | Loss 4.6889 | LR 0.000016\n",
      "Epoch 1 | Step 150/3657 | Loss 4.6406 | LR 0.000025\n",
      "Epoch 1 | Step 200/3657 | Loss 4.5647 | LR 0.000033\n",
      "Epoch 1 | Step 250/3657 | Loss 4.4698 | LR 0.000041\n",
      "Epoch 1 | Step 300/3657 | Loss 4.3560 | LR 0.000049\n",
      "Epoch 1 | Step 350/3657 | Loss 4.2217 | LR 0.000057\n",
      "Epoch 1 | Step 400/3657 | Loss 4.0700 | LR 0.000066\n",
      "Epoch 1 | Step 450/3657 | Loss 3.9054 | LR 0.000074\n",
      "Epoch 1 | Step 500/3657 | Loss 3.7347 | LR 0.000082\n",
      "Epoch 1 | Step 550/3657 | Loss 3.5598 | LR 0.000090\n",
      "Epoch 1 | Step 600/3657 | Loss 3.3842 | LR 0.000098\n",
      "Epoch 1 | Step 650/3657 | Loss 3.2119 | LR 0.000107\n",
      "Epoch 1 | Step 700/3657 | Loss 3.0527 | LR 0.000115\n",
      "Epoch 1 | Step 750/3657 | Loss 2.9050 | LR 0.000123\n",
      "Epoch 1 | Step 800/3657 | Loss 2.7702 | LR 0.000131\n",
      "Epoch 1 | Step 850/3657 | Loss 2.6463 | LR 0.000139\n",
      "Epoch 1 | Step 900/3657 | Loss 2.5305 | LR 0.000148\n",
      "Epoch 1 | Step 950/3657 | Loss 2.4249 | LR 0.000156\n",
      "Epoch 1 | Step 1000/3657 | Loss 2.3286 | LR 0.000164\n",
      "Epoch 1 | Step 1050/3657 | Loss 2.2390 | LR 0.000172\n",
      "Epoch 1 | Step 1100/3657 | Loss 2.1570 | LR 0.000181\n",
      "Epoch 1 | Step 1150/3657 | Loss 2.0804 | LR 0.000189\n",
      "Epoch 1 | Step 1200/3657 | Loss 2.0102 | LR 0.000197\n",
      "Epoch 1 | Step 1250/3657 | Loss 1.9464 | LR 0.000205\n",
      "Epoch 1 | Step 1300/3657 | Loss 1.8864 | LR 0.000213\n",
      "Epoch 1 | Step 1350/3657 | Loss 1.8300 | LR 0.000222\n",
      "Epoch 1 | Step 1400/3657 | Loss 1.7768 | LR 0.000230\n",
      "Epoch 1 | Step 1450/3657 | Loss 1.7274 | LR 0.000238\n",
      "Epoch 1 | Step 1500/3657 | Loss 1.6806 | LR 0.000246\n",
      "Epoch 1 | Step 1550/3657 | Loss 1.6366 | LR 0.000254\n",
      "Epoch 1 | Step 1600/3657 | Loss 1.5945 | LR 0.000263\n",
      "Epoch 1 | Step 1650/3657 | Loss 1.5560 | LR 0.000271\n",
      "Epoch 1 | Step 1700/3657 | Loss 1.5198 | LR 0.000279\n",
      "Epoch 1 | Step 1750/3657 | Loss 1.4854 | LR 0.000287\n",
      "Epoch 1 | Step 1800/3657 | Loss 1.4527 | LR 0.000295\n",
      "Epoch 1 | Step 1850/3657 | Loss 1.4222 | LR 0.000300\n",
      "Epoch 1 | Step 1900/3657 | Loss 1.3938 | LR 0.000300\n",
      "Epoch 1 | Step 1950/3657 | Loss 1.3660 | LR 0.000300\n",
      "Epoch 1 | Step 2000/3657 | Loss 1.3400 | LR 0.000300\n",
      "Epoch 1 | Step 2050/3657 | Loss 1.3158 | LR 0.000300\n",
      "Epoch 1 | Step 2100/3657 | Loss 1.2918 | LR 0.000300\n",
      "Epoch 1 | Step 2150/3657 | Loss 1.2692 | LR 0.000300\n",
      "Epoch 1 | Step 2200/3657 | Loss 1.2471 | LR 0.000300\n",
      "Epoch 1 | Step 2250/3657 | Loss 1.2259 | LR 0.000300\n",
      "Epoch 1 | Step 2300/3657 | Loss 1.2061 | LR 0.000300\n",
      "Epoch 1 | Step 2350/3657 | Loss 1.1862 | LR 0.000300\n",
      "Epoch 1 | Step 2400/3657 | Loss 1.1673 | LR 0.000300\n",
      "Epoch 1 | Step 2450/3657 | Loss 1.1498 | LR 0.000300\n",
      "Epoch 1 | Step 2500/3657 | Loss 1.1321 | LR 0.000300\n",
      "Epoch 1 | Step 2550/3657 | Loss 1.1154 | LR 0.000300\n",
      "Epoch 1 | Step 2600/3657 | Loss 1.0994 | LR 0.000300\n",
      "Epoch 1 | Step 2650/3657 | Loss 1.0843 | LR 0.000300\n",
      "Epoch 1 | Step 2700/3657 | Loss 1.0696 | LR 0.000300\n",
      "Epoch 1 | Step 2750/3657 | Loss 1.0552 | LR 0.000299\n",
      "Epoch 1 | Step 2800/3657 | Loss 1.0414 | LR 0.000299\n",
      "Epoch 1 | Step 2850/3657 | Loss 1.0284 | LR 0.000299\n",
      "Epoch 1 | Step 2900/3657 | Loss 1.0159 | LR 0.000299\n",
      "Epoch 1 | Step 2950/3657 | Loss 1.0042 | LR 0.000299\n",
      "Epoch 1 | Step 3000/3657 | Loss 0.9922 | LR 0.000299\n",
      "Epoch 1 | Step 3050/3657 | Loss 0.9806 | LR 0.000299\n",
      "Epoch 1 | Step 3100/3657 | Loss 0.9691 | LR 0.000299\n",
      "Epoch 1 | Step 3150/3657 | Loss 0.9588 | LR 0.000299\n",
      "Epoch 1 | Step 3200/3657 | Loss 0.9481 | LR 0.000299\n",
      "Epoch 1 | Step 3250/3657 | Loss 0.9381 | LR 0.000299\n",
      "Epoch 1 | Step 3300/3657 | Loss 0.9284 | LR 0.000299\n",
      "Epoch 1 | Step 3350/3657 | Loss 0.9188 | LR 0.000299\n",
      "Epoch 1 | Step 3400/3657 | Loss 0.9092 | LR 0.000298\n",
      "Epoch 1 | Step 3450/3657 | Loss 0.9004 | LR 0.000298\n",
      "Epoch 1 | Step 3500/3657 | Loss 0.8916 | LR 0.000298\n",
      "Epoch 1 | Step 3550/3657 | Loss 0.8831 | LR 0.000298\n",
      "Epoch 1 | Step 3600/3657 | Loss 0.8743 | LR 0.000298\n",
      "Epoch 1 | Step 3650/3657 | Loss 0.8658 | LR 0.000298\n",
      "Epoch 1 | Step 3657/3657 | Loss 0.8647 | LR 0.000298\n",
      "Epoch 1 done -> Val Loss: 0.2708 | Val Acc: 91.48%\n",
      "New best (91.48%). Saved to ./vit-imagenet100-head-only/best\n",
      "Epoch 2 | Step 50/3657 | Loss 0.2474 | LR 0.000298\n",
      "Epoch 2 | Step 100/3657 | Loss 0.2688 | LR 0.000298\n",
      "Epoch 2 | Step 150/3657 | Loss 0.2705 | LR 0.000298\n",
      "Epoch 2 | Step 200/3657 | Loss 0.2603 | LR 0.000297\n",
      "Epoch 2 | Step 250/3657 | Loss 0.2640 | LR 0.000297\n",
      "Epoch 2 | Step 300/3657 | Loss 0.2626 | LR 0.000297\n",
      "Epoch 2 | Step 350/3657 | Loss 0.2639 | LR 0.000297\n",
      "Epoch 2 | Step 400/3657 | Loss 0.2628 | LR 0.000297\n",
      "Epoch 2 | Step 450/3657 | Loss 0.2660 | LR 0.000297\n",
      "Epoch 2 | Step 500/3657 | Loss 0.2651 | LR 0.000297\n",
      "Epoch 2 | Step 550/3657 | Loss 0.2643 | LR 0.000297\n",
      "Epoch 2 | Step 600/3657 | Loss 0.2621 | LR 0.000296\n",
      "Epoch 2 | Step 650/3657 | Loss 0.2634 | LR 0.000296\n",
      "Epoch 2 | Step 700/3657 | Loss 0.2610 | LR 0.000296\n",
      "Epoch 2 | Step 750/3657 | Loss 0.2610 | LR 0.000296\n",
      "Epoch 2 | Step 800/3657 | Loss 0.2572 | LR 0.000296\n",
      "Epoch 2 | Step 850/3657 | Loss 0.2581 | LR 0.000296\n",
      "Epoch 2 | Step 900/3657 | Loss 0.2591 | LR 0.000295\n",
      "Epoch 2 | Step 950/3657 | Loss 0.2594 | LR 0.000295\n",
      "Epoch 2 | Step 1000/3657 | Loss 0.2564 | LR 0.000295\n",
      "Epoch 2 | Step 1050/3657 | Loss 0.2559 | LR 0.000295\n",
      "Epoch 2 | Step 1100/3657 | Loss 0.2562 | LR 0.000295\n",
      "Epoch 2 | Step 1150/3657 | Loss 0.2569 | LR 0.000295\n",
      "Epoch 2 | Step 1200/3657 | Loss 0.2560 | LR 0.000294\n",
      "Epoch 2 | Step 1250/3657 | Loss 0.2565 | LR 0.000294\n",
      "Epoch 2 | Step 1300/3657 | Loss 0.2563 | LR 0.000294\n",
      "Epoch 2 | Step 1350/3657 | Loss 0.2560 | LR 0.000294\n",
      "Epoch 2 | Step 1400/3657 | Loss 0.2567 | LR 0.000294\n",
      "Epoch 2 | Step 1450/3657 | Loss 0.2567 | LR 0.000293\n",
      "Epoch 2 | Step 1500/3657 | Loss 0.2581 | LR 0.000293\n",
      "Epoch 2 | Step 1550/3657 | Loss 0.2585 | LR 0.000293\n",
      "Epoch 2 | Step 1600/3657 | Loss 0.2594 | LR 0.000293\n",
      "Epoch 2 | Step 1650/3657 | Loss 0.2598 | LR 0.000293\n",
      "Epoch 2 | Step 1700/3657 | Loss 0.2597 | LR 0.000292\n",
      "Epoch 2 | Step 1750/3657 | Loss 0.2601 | LR 0.000292\n",
      "Epoch 2 | Step 1800/3657 | Loss 0.2600 | LR 0.000292\n",
      "Epoch 2 | Step 1850/3657 | Loss 0.2600 | LR 0.000292\n",
      "Epoch 2 | Step 1900/3657 | Loss 0.2605 | LR 0.000292\n",
      "Epoch 2 | Step 1950/3657 | Loss 0.2616 | LR 0.000291\n",
      "Epoch 2 | Step 2000/3657 | Loss 0.2612 | LR 0.000291\n",
      "Epoch 2 | Step 2050/3657 | Loss 0.2630 | LR 0.000291\n",
      "Epoch 2 | Step 2100/3657 | Loss 0.2625 | LR 0.000291\n",
      "Epoch 2 | Step 2150/3657 | Loss 0.2626 | LR 0.000290\n",
      "Epoch 2 | Step 2200/3657 | Loss 0.2614 | LR 0.000290\n",
      "Epoch 2 | Step 2250/3657 | Loss 0.2613 | LR 0.000290\n",
      "Epoch 2 | Step 2300/3657 | Loss 0.2615 | LR 0.000290\n",
      "Epoch 2 | Step 2350/3657 | Loss 0.2614 | LR 0.000289\n",
      "Epoch 2 | Step 2400/3657 | Loss 0.2613 | LR 0.000289\n",
      "Epoch 2 | Step 2450/3657 | Loss 0.2612 | LR 0.000289\n",
      "Epoch 2 | Step 2500/3657 | Loss 0.2605 | LR 0.000289\n",
      "Epoch 2 | Step 2550/3657 | Loss 0.2604 | LR 0.000288\n",
      "Epoch 2 | Step 2600/3657 | Loss 0.2601 | LR 0.000288\n",
      "Epoch 2 | Step 2650/3657 | Loss 0.2599 | LR 0.000288\n",
      "Epoch 2 | Step 2700/3657 | Loss 0.2600 | LR 0.000288\n",
      "Epoch 2 | Step 2750/3657 | Loss 0.2599 | LR 0.000287\n",
      "Epoch 2 | Step 2800/3657 | Loss 0.2608 | LR 0.000287\n",
      "Epoch 2 | Step 2850/3657 | Loss 0.2602 | LR 0.000287\n",
      "Epoch 2 | Step 2900/3657 | Loss 0.2600 | LR 0.000286\n",
      "Epoch 2 | Step 2950/3657 | Loss 0.2598 | LR 0.000286\n",
      "Epoch 2 | Step 3000/3657 | Loss 0.2596 | LR 0.000286\n",
      "Epoch 2 | Step 3050/3657 | Loss 0.2591 | LR 0.000286\n",
      "Epoch 2 | Step 3100/3657 | Loss 0.2586 | LR 0.000285\n",
      "Epoch 2 | Step 3150/3657 | Loss 0.2585 | LR 0.000285\n",
      "Epoch 2 | Step 3200/3657 | Loss 0.2587 | LR 0.000285\n",
      "Epoch 2 | Step 3250/3657 | Loss 0.2588 | LR 0.000284\n",
      "Epoch 2 | Step 3300/3657 | Loss 0.2591 | LR 0.000284\n",
      "Epoch 2 | Step 3350/3657 | Loss 0.2593 | LR 0.000284\n",
      "Epoch 2 | Step 3400/3657 | Loss 0.2595 | LR 0.000284\n",
      "Epoch 2 | Step 3450/3657 | Loss 0.2593 | LR 0.000283\n",
      "Epoch 2 | Step 3500/3657 | Loss 0.2597 | LR 0.000283\n",
      "Epoch 2 | Step 3550/3657 | Loss 0.2597 | LR 0.000283\n",
      "Epoch 2 | Step 3600/3657 | Loss 0.2596 | LR 0.000282\n",
      "Epoch 2 | Step 3650/3657 | Loss 0.2590 | LR 0.000282\n",
      "Epoch 2 | Step 3657/3657 | Loss 0.2591 | LR 0.000282\n",
      "Epoch 2 done -> Val Loss: 0.2608 | Val Acc: 91.88%\n",
      "New best (91.88%). Saved to ./vit-imagenet100-head-only/best\n",
      "Epoch 3 | Step 50/3657 | Loss 0.2378 | LR 0.000282\n",
      "Epoch 3 | Step 100/3657 | Loss 0.2265 | LR 0.000281\n",
      "Epoch 3 | Step 150/3657 | Loss 0.2190 | LR 0.000281\n",
      "Epoch 3 | Step 200/3657 | Loss 0.2229 | LR 0.000281\n",
      "Epoch 3 | Step 250/3657 | Loss 0.2224 | LR 0.000280\n",
      "Epoch 3 | Step 300/3657 | Loss 0.2239 | LR 0.000280\n",
      "Epoch 3 | Step 350/3657 | Loss 0.2257 | LR 0.000280\n",
      "Epoch 3 | Step 400/3657 | Loss 0.2254 | LR 0.000279\n",
      "Epoch 3 | Step 450/3657 | Loss 0.2245 | LR 0.000279\n",
      "Epoch 3 | Step 500/3657 | Loss 0.2250 | LR 0.000279\n",
      "Epoch 3 | Step 550/3657 | Loss 0.2255 | LR 0.000278\n",
      "Epoch 3 | Step 600/3657 | Loss 0.2260 | LR 0.000278\n",
      "Epoch 3 | Step 650/3657 | Loss 0.2257 | LR 0.000277\n",
      "Epoch 3 | Step 700/3657 | Loss 0.2260 | LR 0.000277\n",
      "Epoch 3 | Step 750/3657 | Loss 0.2253 | LR 0.000277\n",
      "Epoch 3 | Step 800/3657 | Loss 0.2270 | LR 0.000276\n",
      "Epoch 3 | Step 850/3657 | Loss 0.2254 | LR 0.000276\n",
      "Epoch 3 | Step 900/3657 | Loss 0.2259 | LR 0.000276\n",
      "Epoch 3 | Step 950/3657 | Loss 0.2261 | LR 0.000275\n",
      "Epoch 3 | Step 1000/3657 | Loss 0.2253 | LR 0.000275\n",
      "Epoch 3 | Step 1050/3657 | Loss 0.2264 | LR 0.000275\n",
      "Epoch 3 | Step 1100/3657 | Loss 0.2258 | LR 0.000274\n",
      "Epoch 3 | Step 1150/3657 | Loss 0.2248 | LR 0.000274\n",
      "Epoch 3 | Step 1200/3657 | Loss 0.2261 | LR 0.000273\n",
      "Epoch 3 | Step 1250/3657 | Loss 0.2269 | LR 0.000273\n",
      "Epoch 3 | Step 1300/3657 | Loss 0.2266 | LR 0.000273\n",
      "Epoch 3 | Step 1350/3657 | Loss 0.2275 | LR 0.000272\n",
      "Epoch 3 | Step 1400/3657 | Loss 0.2270 | LR 0.000272\n",
      "Epoch 3 | Step 1450/3657 | Loss 0.2278 | LR 0.000271\n",
      "Epoch 3 | Step 1500/3657 | Loss 0.2278 | LR 0.000271\n",
      "Epoch 3 | Step 1550/3657 | Loss 0.2268 | LR 0.000271\n",
      "Epoch 3 | Step 1600/3657 | Loss 0.2274 | LR 0.000270\n",
      "Epoch 3 | Step 1650/3657 | Loss 0.2273 | LR 0.000270\n",
      "Epoch 3 | Step 1700/3657 | Loss 0.2285 | LR 0.000269\n",
      "Epoch 3 | Step 1750/3657 | Loss 0.2275 | LR 0.000269\n",
      "Epoch 3 | Step 1800/3657 | Loss 0.2280 | LR 0.000269\n",
      "Epoch 3 | Step 1850/3657 | Loss 0.2291 | LR 0.000268\n",
      "Epoch 3 | Step 1900/3657 | Loss 0.2288 | LR 0.000268\n",
      "Epoch 3 | Step 1950/3657 | Loss 0.2290 | LR 0.000267\n",
      "Epoch 3 | Step 2000/3657 | Loss 0.2296 | LR 0.000267\n",
      "Epoch 3 | Step 2050/3657 | Loss 0.2299 | LR 0.000266\n",
      "Epoch 3 | Step 2100/3657 | Loss 0.2293 | LR 0.000266\n",
      "Epoch 3 | Step 2150/3657 | Loss 0.2294 | LR 0.000266\n",
      "Epoch 3 | Step 2200/3657 | Loss 0.2294 | LR 0.000265\n",
      "Epoch 3 | Step 2250/3657 | Loss 0.2293 | LR 0.000265\n",
      "Epoch 3 | Step 2300/3657 | Loss 0.2297 | LR 0.000264\n",
      "Epoch 3 | Step 2350/3657 | Loss 0.2295 | LR 0.000264\n",
      "Epoch 3 | Step 2400/3657 | Loss 0.2301 | LR 0.000263\n",
      "Epoch 3 | Step 2450/3657 | Loss 0.2301 | LR 0.000263\n",
      "Epoch 3 | Step 2500/3657 | Loss 0.2299 | LR 0.000263\n",
      "Epoch 3 | Step 2550/3657 | Loss 0.2302 | LR 0.000262\n",
      "Epoch 3 | Step 2600/3657 | Loss 0.2306 | LR 0.000262\n",
      "Epoch 3 | Step 2650/3657 | Loss 0.2311 | LR 0.000261\n",
      "Epoch 3 | Step 2700/3657 | Loss 0.2314 | LR 0.000261\n",
      "Epoch 3 | Step 2750/3657 | Loss 0.2311 | LR 0.000260\n",
      "Epoch 3 | Step 2800/3657 | Loss 0.2310 | LR 0.000260\n",
      "Epoch 3 | Step 2850/3657 | Loss 0.2313 | LR 0.000259\n",
      "Epoch 3 | Step 2900/3657 | Loss 0.2315 | LR 0.000259\n",
      "Epoch 3 | Step 2950/3657 | Loss 0.2308 | LR 0.000258\n",
      "Epoch 3 | Step 3000/3657 | Loss 0.2311 | LR 0.000258\n",
      "Epoch 3 | Step 3050/3657 | Loss 0.2310 | LR 0.000257\n",
      "Epoch 3 | Step 3100/3657 | Loss 0.2310 | LR 0.000257\n",
      "Epoch 3 | Step 3150/3657 | Loss 0.2303 | LR 0.000257\n",
      "Epoch 3 | Step 3200/3657 | Loss 0.2303 | LR 0.000256\n",
      "Epoch 3 | Step 3250/3657 | Loss 0.2302 | LR 0.000256\n",
      "Epoch 3 | Step 3300/3657 | Loss 0.2301 | LR 0.000255\n",
      "Epoch 3 | Step 3350/3657 | Loss 0.2299 | LR 0.000255\n",
      "Epoch 3 | Step 3400/3657 | Loss 0.2303 | LR 0.000254\n",
      "Epoch 3 | Step 3450/3657 | Loss 0.2304 | LR 0.000254\n",
      "Epoch 3 | Step 3500/3657 | Loss 0.2304 | LR 0.000253\n",
      "Epoch 3 | Step 3550/3657 | Loss 0.2300 | LR 0.000253\n",
      "Epoch 3 | Step 3600/3657 | Loss 0.2297 | LR 0.000252\n",
      "Epoch 3 | Step 3650/3657 | Loss 0.2294 | LR 0.000252\n",
      "Epoch 3 | Step 3657/3657 | Loss 0.2292 | LR 0.000252\n",
      "Epoch 3 done -> Val Loss: 0.2607 | Val Acc: 91.89%\n",
      "New best (91.89%). Saved to ./vit-imagenet100-head-only/best\n",
      "Epoch 4 | Step 50/3657 | Loss 0.1903 | LR 0.000251\n",
      "Epoch 4 | Step 100/3657 | Loss 0.1998 | LR 0.000251\n",
      "Epoch 4 | Step 150/3657 | Loss 0.1996 | LR 0.000250\n",
      "Epoch 4 | Step 200/3657 | Loss 0.2056 | LR 0.000250\n",
      "Epoch 4 | Step 250/3657 | Loss 0.2048 | LR 0.000249\n",
      "Epoch 4 | Step 300/3657 | Loss 0.2024 | LR 0.000249\n",
      "Epoch 4 | Step 350/3657 | Loss 0.2022 | LR 0.000248\n",
      "Epoch 4 | Step 400/3657 | Loss 0.2032 | LR 0.000248\n",
      "Epoch 4 | Step 450/3657 | Loss 0.2070 | LR 0.000247\n",
      "Epoch 4 | Step 500/3657 | Loss 0.2065 | LR 0.000246\n",
      "Epoch 4 | Step 550/3657 | Loss 0.2075 | LR 0.000246\n",
      "Epoch 4 | Step 600/3657 | Loss 0.2063 | LR 0.000245\n",
      "Epoch 4 | Step 650/3657 | Loss 0.2072 | LR 0.000245\n",
      "Epoch 4 | Step 700/3657 | Loss 0.2084 | LR 0.000244\n",
      "Epoch 4 | Step 750/3657 | Loss 0.2086 | LR 0.000244\n",
      "Epoch 4 | Step 800/3657 | Loss 0.2067 | LR 0.000243\n",
      "Epoch 4 | Step 850/3657 | Loss 0.2060 | LR 0.000243\n",
      "Epoch 4 | Step 900/3657 | Loss 0.2062 | LR 0.000242\n",
      "Epoch 4 | Step 950/3657 | Loss 0.2080 | LR 0.000242\n",
      "Epoch 4 | Step 1000/3657 | Loss 0.2082 | LR 0.000241\n",
      "Epoch 4 | Step 1050/3657 | Loss 0.2084 | LR 0.000241\n",
      "Epoch 4 | Step 1100/3657 | Loss 0.2071 | LR 0.000240\n",
      "Epoch 4 | Step 1150/3657 | Loss 0.2077 | LR 0.000240\n",
      "Epoch 4 | Step 1200/3657 | Loss 0.2062 | LR 0.000239\n",
      "Epoch 4 | Step 1250/3657 | Loss 0.2056 | LR 0.000238\n",
      "Epoch 4 | Step 1300/3657 | Loss 0.2060 | LR 0.000238\n",
      "Epoch 4 | Step 1350/3657 | Loss 0.2057 | LR 0.000237\n",
      "Epoch 4 | Step 1400/3657 | Loss 0.2070 | LR 0.000237\n",
      "Epoch 4 | Step 1450/3657 | Loss 0.2070 | LR 0.000236\n",
      "Epoch 4 | Step 1500/3657 | Loss 0.2066 | LR 0.000236\n",
      "Epoch 4 | Step 1550/3657 | Loss 0.2068 | LR 0.000235\n",
      "Epoch 4 | Step 1600/3657 | Loss 0.2075 | LR 0.000235\n",
      "Epoch 4 | Step 1650/3657 | Loss 0.2078 | LR 0.000234\n",
      "Epoch 4 | Step 1700/3657 | Loss 0.2069 | LR 0.000233\n",
      "Epoch 4 | Step 1750/3657 | Loss 0.2070 | LR 0.000233\n",
      "Epoch 4 | Step 1800/3657 | Loss 0.2064 | LR 0.000232\n",
      "Epoch 4 | Step 1850/3657 | Loss 0.2063 | LR 0.000232\n",
      "Epoch 4 | Step 1900/3657 | Loss 0.2058 | LR 0.000231\n",
      "Epoch 4 | Step 1950/3657 | Loss 0.2064 | LR 0.000231\n",
      "Epoch 4 | Step 2000/3657 | Loss 0.2065 | LR 0.000230\n",
      "Epoch 4 | Step 2050/3657 | Loss 0.2068 | LR 0.000230\n",
      "Epoch 4 | Step 2100/3657 | Loss 0.2073 | LR 0.000229\n",
      "Epoch 4 | Step 2150/3657 | Loss 0.2080 | LR 0.000228\n",
      "Epoch 4 | Step 2200/3657 | Loss 0.2085 | LR 0.000228\n",
      "Epoch 4 | Step 2250/3657 | Loss 0.2087 | LR 0.000227\n",
      "Epoch 4 | Step 2300/3657 | Loss 0.2093 | LR 0.000227\n",
      "Epoch 4 | Step 2350/3657 | Loss 0.2092 | LR 0.000226\n",
      "Epoch 4 | Step 2400/3657 | Loss 0.2092 | LR 0.000225\n",
      "Epoch 4 | Step 2450/3657 | Loss 0.2090 | LR 0.000225\n",
      "Epoch 4 | Step 2500/3657 | Loss 0.2093 | LR 0.000224\n",
      "Epoch 4 | Step 2550/3657 | Loss 0.2092 | LR 0.000224\n",
      "Epoch 4 | Step 2600/3657 | Loss 0.2094 | LR 0.000223\n",
      "Epoch 4 | Step 2650/3657 | Loss 0.2094 | LR 0.000222\n",
      "Epoch 4 | Step 2700/3657 | Loss 0.2090 | LR 0.000222\n",
      "Epoch 4 | Step 2750/3657 | Loss 0.2089 | LR 0.000221\n",
      "Epoch 4 | Step 2800/3657 | Loss 0.2092 | LR 0.000221\n",
      "Epoch 4 | Step 2850/3657 | Loss 0.2094 | LR 0.000220\n",
      "Epoch 4 | Step 2900/3657 | Loss 0.2092 | LR 0.000220\n",
      "Epoch 4 | Step 2950/3657 | Loss 0.2091 | LR 0.000219\n",
      "Epoch 4 | Step 3000/3657 | Loss 0.2092 | LR 0.000218\n",
      "Epoch 4 | Step 3050/3657 | Loss 0.2090 | LR 0.000218\n",
      "Epoch 4 | Step 3100/3657 | Loss 0.2094 | LR 0.000217\n",
      "Epoch 4 | Step 3150/3657 | Loss 0.2090 | LR 0.000216\n",
      "Epoch 4 | Step 3200/3657 | Loss 0.2087 | LR 0.000216\n",
      "Epoch 4 | Step 3250/3657 | Loss 0.2091 | LR 0.000215\n",
      "Epoch 4 | Step 3300/3657 | Loss 0.2090 | LR 0.000215\n",
      "Epoch 4 | Step 3350/3657 | Loss 0.2090 | LR 0.000214\n",
      "Epoch 4 | Step 3400/3657 | Loss 0.2091 | LR 0.000213\n",
      "Epoch 4 | Step 3450/3657 | Loss 0.2089 | LR 0.000213\n",
      "Epoch 4 | Step 3500/3657 | Loss 0.2089 | LR 0.000212\n",
      "Epoch 4 | Step 3550/3657 | Loss 0.2091 | LR 0.000212\n",
      "Epoch 4 | Step 3600/3657 | Loss 0.2091 | LR 0.000211\n",
      "Epoch 4 | Step 3650/3657 | Loss 0.2091 | LR 0.000210\n",
      "Epoch 4 | Step 3657/3657 | Loss 0.2092 | LR 0.000210\n",
      "Epoch 4 done -> Val Loss: 0.2585 | Val Acc: 92.02%\n",
      "New best (92.02%). Saved to ./vit-imagenet100-head-only/best\n",
      "Epoch 5 | Step 50/3657 | Loss 0.1840 | LR 0.000210\n",
      "Epoch 5 | Step 100/3657 | Loss 0.1908 | LR 0.000209\n",
      "Epoch 5 | Step 150/3657 | Loss 0.1913 | LR 0.000208\n",
      "Epoch 5 | Step 200/3657 | Loss 0.1858 | LR 0.000208\n",
      "Epoch 5 | Step 250/3657 | Loss 0.1884 | LR 0.000207\n",
      "Epoch 5 | Step 300/3657 | Loss 0.1877 | LR 0.000207\n",
      "Epoch 5 | Step 350/3657 | Loss 0.1864 | LR 0.000206\n",
      "Epoch 5 | Step 400/3657 | Loss 0.1875 | LR 0.000205\n",
      "Epoch 5 | Step 450/3657 | Loss 0.1873 | LR 0.000205\n",
      "Epoch 5 | Step 500/3657 | Loss 0.1877 | LR 0.000204\n",
      "Epoch 5 | Step 550/3657 | Loss 0.1864 | LR 0.000203\n",
      "Epoch 5 | Step 600/3657 | Loss 0.1883 | LR 0.000203\n",
      "Epoch 5 | Step 650/3657 | Loss 0.1869 | LR 0.000202\n",
      "Epoch 5 | Step 700/3657 | Loss 0.1885 | LR 0.000201\n",
      "Epoch 5 | Step 750/3657 | Loss 0.1866 | LR 0.000201\n",
      "Epoch 5 | Step 800/3657 | Loss 0.1865 | LR 0.000200\n",
      "Epoch 5 | Step 850/3657 | Loss 0.1858 | LR 0.000200\n",
      "Epoch 5 | Step 900/3657 | Loss 0.1861 | LR 0.000199\n",
      "Epoch 5 | Step 950/3657 | Loss 0.1869 | LR 0.000198\n",
      "Epoch 5 | Step 1000/3657 | Loss 0.1861 | LR 0.000198\n",
      "Epoch 5 | Step 1050/3657 | Loss 0.1870 | LR 0.000197\n",
      "Epoch 5 | Step 1100/3657 | Loss 0.1877 | LR 0.000196\n",
      "Epoch 5 | Step 1150/3657 | Loss 0.1872 | LR 0.000196\n",
      "Epoch 5 | Step 1200/3657 | Loss 0.1873 | LR 0.000195\n",
      "Epoch 5 | Step 1250/3657 | Loss 0.1884 | LR 0.000194\n",
      "Epoch 5 | Step 1300/3657 | Loss 0.1885 | LR 0.000194\n",
      "Epoch 5 | Step 1350/3657 | Loss 0.1884 | LR 0.000193\n",
      "Epoch 5 | Step 1400/3657 | Loss 0.1877 | LR 0.000192\n",
      "Epoch 5 | Step 1450/3657 | Loss 0.1879 | LR 0.000192\n",
      "Epoch 5 | Step 1500/3657 | Loss 0.1887 | LR 0.000191\n",
      "Epoch 5 | Step 1550/3657 | Loss 0.1896 | LR 0.000190\n",
      "Epoch 5 | Step 1600/3657 | Loss 0.1896 | LR 0.000190\n",
      "Epoch 5 | Step 1650/3657 | Loss 0.1904 | LR 0.000189\n",
      "Epoch 5 | Step 1700/3657 | Loss 0.1909 | LR 0.000189\n",
      "Epoch 5 | Step 1750/3657 | Loss 0.1922 | LR 0.000188\n",
      "Epoch 5 | Step 1800/3657 | Loss 0.1927 | LR 0.000187\n",
      "Epoch 5 | Step 1850/3657 | Loss 0.1927 | LR 0.000187\n",
      "Epoch 5 | Step 1900/3657 | Loss 0.1925 | LR 0.000186\n",
      "Epoch 5 | Step 1950/3657 | Loss 0.1924 | LR 0.000185\n",
      "Epoch 5 | Step 2000/3657 | Loss 0.1917 | LR 0.000185\n",
      "Epoch 5 | Step 2050/3657 | Loss 0.1922 | LR 0.000184\n",
      "Epoch 5 | Step 2100/3657 | Loss 0.1919 | LR 0.000183\n",
      "Epoch 5 | Step 2150/3657 | Loss 0.1923 | LR 0.000183\n",
      "Epoch 5 | Step 2200/3657 | Loss 0.1920 | LR 0.000182\n",
      "Epoch 5 | Step 2250/3657 | Loss 0.1923 | LR 0.000181\n",
      "Epoch 5 | Step 2300/3657 | Loss 0.1917 | LR 0.000181\n",
      "Epoch 5 | Step 2350/3657 | Loss 0.1918 | LR 0.000180\n",
      "Epoch 5 | Step 2400/3657 | Loss 0.1918 | LR 0.000179\n",
      "Epoch 5 | Step 2450/3657 | Loss 0.1923 | LR 0.000179\n",
      "Epoch 5 | Step 2500/3657 | Loss 0.1923 | LR 0.000178\n",
      "Epoch 5 | Step 2550/3657 | Loss 0.1926 | LR 0.000177\n",
      "Epoch 5 | Step 2600/3657 | Loss 0.1921 | LR 0.000177\n",
      "Epoch 5 | Step 2650/3657 | Loss 0.1929 | LR 0.000176\n",
      "Epoch 5 | Step 2700/3657 | Loss 0.1936 | LR 0.000175\n",
      "Epoch 5 | Step 2750/3657 | Loss 0.1935 | LR 0.000175\n",
      "Epoch 5 | Step 2800/3657 | Loss 0.1934 | LR 0.000174\n",
      "Epoch 5 | Step 2850/3657 | Loss 0.1938 | LR 0.000173\n",
      "Epoch 5 | Step 2900/3657 | Loss 0.1938 | LR 0.000173\n",
      "Epoch 5 | Step 2950/3657 | Loss 0.1940 | LR 0.000172\n",
      "Epoch 5 | Step 3000/3657 | Loss 0.1941 | LR 0.000171\n",
      "Epoch 5 | Step 3050/3657 | Loss 0.1938 | LR 0.000171\n",
      "Epoch 5 | Step 3100/3657 | Loss 0.1941 | LR 0.000170\n",
      "Epoch 5 | Step 3150/3657 | Loss 0.1938 | LR 0.000169\n",
      "Epoch 5 | Step 3200/3657 | Loss 0.1936 | LR 0.000169\n",
      "Epoch 5 | Step 3250/3657 | Loss 0.1932 | LR 0.000168\n",
      "Epoch 5 | Step 3300/3657 | Loss 0.1936 | LR 0.000167\n",
      "Epoch 5 | Step 3350/3657 | Loss 0.1934 | LR 0.000167\n",
      "Epoch 5 | Step 3400/3657 | Loss 0.1936 | LR 0.000166\n",
      "Epoch 5 | Step 3450/3657 | Loss 0.1937 | LR 0.000165\n",
      "Epoch 5 | Step 3500/3657 | Loss 0.1943 | LR 0.000165\n",
      "Epoch 5 | Step 3550/3657 | Loss 0.1948 | LR 0.000164\n",
      "Epoch 5 | Step 3600/3657 | Loss 0.1950 | LR 0.000163\n",
      "Epoch 5 | Step 3650/3657 | Loss 0.1952 | LR 0.000162\n",
      "Epoch 5 | Step 3657/3657 | Loss 0.1951 | LR 0.000162\n",
      "Epoch 5 done -> Val Loss: 0.2606 | Val Acc: 91.92%\n",
      "Epoch 6 | Step 50/3657 | Loss 0.1408 | LR 0.000162\n",
      "Epoch 6 | Step 100/3657 | Loss 0.1786 | LR 0.000161\n",
      "Epoch 6 | Step 150/3657 | Loss 0.1801 | LR 0.000160\n",
      "Epoch 6 | Step 200/3657 | Loss 0.1856 | LR 0.000160\n",
      "Epoch 6 | Step 250/3657 | Loss 0.1831 | LR 0.000159\n",
      "Epoch 6 | Step 300/3657 | Loss 0.1837 | LR 0.000158\n",
      "Epoch 6 | Step 350/3657 | Loss 0.1811 | LR 0.000158\n",
      "Epoch 6 | Step 400/3657 | Loss 0.1803 | LR 0.000157\n",
      "Epoch 6 | Step 450/3657 | Loss 0.1824 | LR 0.000156\n",
      "Epoch 6 | Step 500/3657 | Loss 0.1828 | LR 0.000156\n",
      "Epoch 6 | Step 550/3657 | Loss 0.1828 | LR 0.000155\n",
      "Epoch 6 | Step 600/3657 | Loss 0.1831 | LR 0.000154\n",
      "Epoch 6 | Step 650/3657 | Loss 0.1824 | LR 0.000154\n",
      "Epoch 6 | Step 700/3657 | Loss 0.1839 | LR 0.000153\n",
      "Epoch 6 | Step 750/3657 | Loss 0.1817 | LR 0.000152\n",
      "Epoch 6 | Step 800/3657 | Loss 0.1812 | LR 0.000152\n",
      "Epoch 6 | Step 850/3657 | Loss 0.1817 | LR 0.000151\n",
      "Epoch 6 | Step 900/3657 | Loss 0.1822 | LR 0.000150\n",
      "Epoch 6 | Step 950/3657 | Loss 0.1824 | LR 0.000150\n",
      "Epoch 6 | Step 1000/3657 | Loss 0.1827 | LR 0.000149\n",
      "Epoch 6 | Step 1050/3657 | Loss 0.1829 | LR 0.000148\n",
      "Epoch 6 | Step 1100/3657 | Loss 0.1826 | LR 0.000147\n",
      "Epoch 6 | Step 1150/3657 | Loss 0.1822 | LR 0.000147\n",
      "Epoch 6 | Step 1200/3657 | Loss 0.1824 | LR 0.000146\n",
      "Epoch 6 | Step 1250/3657 | Loss 0.1822 | LR 0.000145\n",
      "Epoch 6 | Step 1300/3657 | Loss 0.1818 | LR 0.000145\n",
      "Epoch 6 | Step 1350/3657 | Loss 0.1821 | LR 0.000144\n",
      "Epoch 6 | Step 1400/3657 | Loss 0.1819 | LR 0.000143\n",
      "Epoch 6 | Step 1450/3657 | Loss 0.1824 | LR 0.000143\n",
      "Epoch 6 | Step 1500/3657 | Loss 0.1825 | LR 0.000142\n",
      "Epoch 6 | Step 1550/3657 | Loss 0.1820 | LR 0.000141\n",
      "Epoch 6 | Step 1600/3657 | Loss 0.1823 | LR 0.000141\n",
      "Epoch 6 | Step 1650/3657 | Loss 0.1827 | LR 0.000140\n",
      "Epoch 6 | Step 1700/3657 | Loss 0.1836 | LR 0.000139\n",
      "Epoch 6 | Step 1750/3657 | Loss 0.1837 | LR 0.000139\n",
      "Epoch 6 | Step 1800/3657 | Loss 0.1836 | LR 0.000138\n",
      "Epoch 6 | Step 1850/3657 | Loss 0.1834 | LR 0.000137\n",
      "Epoch 6 | Step 1900/3657 | Loss 0.1831 | LR 0.000137\n",
      "Epoch 6 | Step 1950/3657 | Loss 0.1832 | LR 0.000136\n",
      "Epoch 6 | Step 2000/3657 | Loss 0.1830 | LR 0.000135\n",
      "Epoch 6 | Step 2050/3657 | Loss 0.1834 | LR 0.000135\n",
      "Epoch 6 | Step 2100/3657 | Loss 0.1829 | LR 0.000134\n",
      "Epoch 6 | Step 2150/3657 | Loss 0.1830 | LR 0.000133\n",
      "Epoch 6 | Step 2200/3657 | Loss 0.1831 | LR 0.000133\n",
      "Epoch 6 | Step 2250/3657 | Loss 0.1834 | LR 0.000132\n",
      "Epoch 6 | Step 2300/3657 | Loss 0.1834 | LR 0.000131\n",
      "Epoch 6 | Step 2350/3657 | Loss 0.1837 | LR 0.000131\n",
      "Epoch 6 | Step 2400/3657 | Loss 0.1840 | LR 0.000130\n",
      "Epoch 6 | Step 2450/3657 | Loss 0.1842 | LR 0.000129\n",
      "Epoch 6 | Step 2500/3657 | Loss 0.1843 | LR 0.000129\n",
      "Epoch 6 | Step 2550/3657 | Loss 0.1839 | LR 0.000128\n",
      "Epoch 6 | Step 2600/3657 | Loss 0.1842 | LR 0.000127\n",
      "Epoch 6 | Step 2650/3657 | Loss 0.1848 | LR 0.000127\n",
      "Epoch 6 | Step 2700/3657 | Loss 0.1852 | LR 0.000126\n",
      "Epoch 6 | Step 2750/3657 | Loss 0.1857 | LR 0.000125\n",
      "Epoch 6 | Step 2800/3657 | Loss 0.1854 | LR 0.000125\n",
      "Epoch 6 | Step 2850/3657 | Loss 0.1850 | LR 0.000124\n",
      "Epoch 6 | Step 2900/3657 | Loss 0.1847 | LR 0.000123\n",
      "Epoch 6 | Step 2950/3657 | Loss 0.1846 | LR 0.000123\n",
      "Epoch 6 | Step 3000/3657 | Loss 0.1843 | LR 0.000122\n",
      "Epoch 6 | Step 3050/3657 | Loss 0.1841 | LR 0.000121\n",
      "Epoch 6 | Step 3100/3657 | Loss 0.1845 | LR 0.000121\n",
      "Epoch 6 | Step 3150/3657 | Loss 0.1845 | LR 0.000120\n",
      "Epoch 6 | Step 3200/3657 | Loss 0.1836 | LR 0.000119\n",
      "Epoch 6 | Step 3250/3657 | Loss 0.1836 | LR 0.000119\n",
      "Epoch 6 | Step 3300/3657 | Loss 0.1838 | LR 0.000118\n",
      "Epoch 6 | Step 3350/3657 | Loss 0.1837 | LR 0.000117\n",
      "Epoch 6 | Step 3400/3657 | Loss 0.1835 | LR 0.000117\n",
      "Epoch 6 | Step 3450/3657 | Loss 0.1837 | LR 0.000116\n",
      "Epoch 6 | Step 3500/3657 | Loss 0.1837 | LR 0.000115\n",
      "Epoch 6 | Step 3550/3657 | Loss 0.1838 | LR 0.000115\n",
      "Epoch 6 | Step 3600/3657 | Loss 0.1835 | LR 0.000114\n",
      "Epoch 6 | Step 3650/3657 | Loss 0.1837 | LR 0.000113\n",
      "Epoch 6 | Step 3657/3657 | Loss 0.1837 | LR 0.000113\n",
      "Epoch 6 done -> Val Loss: 0.2567 | Val Acc: 92.19%\n",
      "New best (92.19%). Saved to ./vit-imagenet100-head-only/best\n",
      "Epoch 7 | Step 50/3657 | Loss 0.1441 | LR 0.000113\n",
      "Epoch 7 | Step 100/3657 | Loss 0.1466 | LR 0.000112\n",
      "Epoch 7 | Step 150/3657 | Loss 0.1601 | LR 0.000111\n",
      "Epoch 7 | Step 200/3657 | Loss 0.1606 | LR 0.000111\n",
      "Epoch 7 | Step 250/3657 | Loss 0.1643 | LR 0.000110\n",
      "Epoch 7 | Step 300/3657 | Loss 0.1633 | LR 0.000109\n",
      "Epoch 7 | Step 350/3657 | Loss 0.1616 | LR 0.000109\n",
      "Epoch 7 | Step 400/3657 | Loss 0.1662 | LR 0.000108\n",
      "Epoch 7 | Step 450/3657 | Loss 0.1647 | LR 0.000107\n",
      "Epoch 7 | Step 500/3657 | Loss 0.1635 | LR 0.000107\n",
      "Epoch 7 | Step 550/3657 | Loss 0.1646 | LR 0.000106\n",
      "Epoch 7 | Step 600/3657 | Loss 0.1643 | LR 0.000105\n",
      "Epoch 7 | Step 650/3657 | Loss 0.1659 | LR 0.000105\n",
      "Epoch 7 | Step 700/3657 | Loss 0.1667 | LR 0.000104\n",
      "Epoch 7 | Step 750/3657 | Loss 0.1689 | LR 0.000103\n",
      "Epoch 7 | Step 800/3657 | Loss 0.1708 | LR 0.000103\n",
      "Epoch 7 | Step 850/3657 | Loss 0.1710 | LR 0.000102\n",
      "Epoch 7 | Step 900/3657 | Loss 0.1700 | LR 0.000101\n",
      "Epoch 7 | Step 950/3657 | Loss 0.1707 | LR 0.000101\n",
      "Epoch 7 | Step 1000/3657 | Loss 0.1718 | LR 0.000100\n",
      "Epoch 7 | Step 1050/3657 | Loss 0.1723 | LR 0.000100\n",
      "Epoch 7 | Step 1100/3657 | Loss 0.1735 | LR 0.000099\n",
      "Epoch 7 | Step 1150/3657 | Loss 0.1737 | LR 0.000098\n",
      "Epoch 7 | Step 1200/3657 | Loss 0.1735 | LR 0.000098\n",
      "Epoch 7 | Step 1250/3657 | Loss 0.1737 | LR 0.000097\n",
      "Epoch 7 | Step 1300/3657 | Loss 0.1737 | LR 0.000096\n",
      "Epoch 7 | Step 1350/3657 | Loss 0.1744 | LR 0.000096\n",
      "Epoch 7 | Step 1400/3657 | Loss 0.1738 | LR 0.000095\n",
      "Epoch 7 | Step 1450/3657 | Loss 0.1737 | LR 0.000094\n",
      "Epoch 7 | Step 1500/3657 | Loss 0.1736 | LR 0.000094\n",
      "Epoch 7 | Step 1550/3657 | Loss 0.1736 | LR 0.000093\n",
      "Epoch 7 | Step 1600/3657 | Loss 0.1727 | LR 0.000093\n",
      "Epoch 7 | Step 1650/3657 | Loss 0.1730 | LR 0.000092\n",
      "Epoch 7 | Step 1700/3657 | Loss 0.1732 | LR 0.000091\n",
      "Epoch 7 | Step 1750/3657 | Loss 0.1732 | LR 0.000091\n",
      "Epoch 7 | Step 1800/3657 | Loss 0.1729 | LR 0.000090\n",
      "Epoch 7 | Step 1850/3657 | Loss 0.1736 | LR 0.000089\n",
      "Epoch 7 | Step 1900/3657 | Loss 0.1729 | LR 0.000089\n",
      "Epoch 7 | Step 1950/3657 | Loss 0.1730 | LR 0.000088\n",
      "Epoch 7 | Step 2000/3657 | Loss 0.1730 | LR 0.000088\n",
      "Epoch 7 | Step 2050/3657 | Loss 0.1727 | LR 0.000087\n",
      "Epoch 7 | Step 2100/3657 | Loss 0.1723 | LR 0.000086\n",
      "Epoch 7 | Step 2150/3657 | Loss 0.1724 | LR 0.000086\n",
      "Epoch 7 | Step 2200/3657 | Loss 0.1732 | LR 0.000085\n",
      "Epoch 7 | Step 2250/3657 | Loss 0.1729 | LR 0.000085\n",
      "Epoch 7 | Step 2300/3657 | Loss 0.1729 | LR 0.000084\n",
      "Epoch 7 | Step 2350/3657 | Loss 0.1732 | LR 0.000083\n",
      "Epoch 7 | Step 2400/3657 | Loss 0.1736 | LR 0.000083\n",
      "Epoch 7 | Step 2450/3657 | Loss 0.1745 | LR 0.000082\n",
      "Epoch 7 | Step 2500/3657 | Loss 0.1748 | LR 0.000082\n",
      "Epoch 7 | Step 2550/3657 | Loss 0.1751 | LR 0.000081\n",
      "Epoch 7 | Step 2600/3657 | Loss 0.1751 | LR 0.000080\n",
      "Epoch 7 | Step 2650/3657 | Loss 0.1749 | LR 0.000080\n",
      "Epoch 7 | Step 2700/3657 | Loss 0.1747 | LR 0.000079\n",
      "Epoch 7 | Step 2750/3657 | Loss 0.1752 | LR 0.000079\n",
      "Epoch 7 | Step 2800/3657 | Loss 0.1754 | LR 0.000078\n",
      "Epoch 7 | Step 2850/3657 | Loss 0.1754 | LR 0.000077\n",
      "Epoch 7 | Step 2900/3657 | Loss 0.1754 | LR 0.000077\n",
      "Epoch 7 | Step 2950/3657 | Loss 0.1755 | LR 0.000076\n",
      "Epoch 7 | Step 3000/3657 | Loss 0.1754 | LR 0.000076\n",
      "Epoch 7 | Step 3050/3657 | Loss 0.1753 | LR 0.000075\n",
      "Epoch 7 | Step 3100/3657 | Loss 0.1752 | LR 0.000074\n",
      "Epoch 7 | Step 3150/3657 | Loss 0.1754 | LR 0.000074\n",
      "Epoch 7 | Step 3200/3657 | Loss 0.1757 | LR 0.000073\n",
      "Epoch 7 | Step 3250/3657 | Loss 0.1754 | LR 0.000073\n",
      "Epoch 7 | Step 3300/3657 | Loss 0.1755 | LR 0.000072\n",
      "Epoch 7 | Step 3350/3657 | Loss 0.1753 | LR 0.000071\n",
      "Epoch 7 | Step 3400/3657 | Loss 0.1759 | LR 0.000071\n",
      "Epoch 7 | Step 3450/3657 | Loss 0.1761 | LR 0.000070\n",
      "Epoch 7 | Step 3500/3657 | Loss 0.1758 | LR 0.000070\n",
      "Epoch 7 | Step 3550/3657 | Loss 0.1758 | LR 0.000069\n",
      "Epoch 7 | Step 3600/3657 | Loss 0.1756 | LR 0.000069\n",
      "Epoch 7 | Step 3650/3657 | Loss 0.1754 | LR 0.000068\n",
      "Epoch 7 | Step 3657/3657 | Loss 0.1752 | LR 0.000068\n",
      "Epoch 7 done -> Val Loss: 0.2567 | Val Acc: 92.01%\n",
      "Epoch 8 | Step 50/3657 | Loss 0.1696 | LR 0.000067\n",
      "Epoch 8 | Step 100/3657 | Loss 0.1725 | LR 0.000067\n",
      "Epoch 8 | Step 150/3657 | Loss 0.1744 | LR 0.000066\n",
      "Epoch 8 | Step 200/3657 | Loss 0.1677 | LR 0.000066\n",
      "Epoch 8 | Step 250/3657 | Loss 0.1751 | LR 0.000065\n",
      "Epoch 8 | Step 300/3657 | Loss 0.1720 | LR 0.000065\n",
      "Epoch 8 | Step 350/3657 | Loss 0.1687 | LR 0.000064\n",
      "Epoch 8 | Step 400/3657 | Loss 0.1669 | LR 0.000063\n",
      "Epoch 8 | Step 450/3657 | Loss 0.1662 | LR 0.000063\n",
      "Epoch 8 | Step 500/3657 | Loss 0.1653 | LR 0.000062\n",
      "Epoch 8 | Step 550/3657 | Loss 0.1651 | LR 0.000062\n",
      "Epoch 8 | Step 600/3657 | Loss 0.1642 | LR 0.000061\n",
      "Epoch 8 | Step 650/3657 | Loss 0.1639 | LR 0.000061\n",
      "Epoch 8 | Step 700/3657 | Loss 0.1636 | LR 0.000060\n",
      "Epoch 8 | Step 750/3657 | Loss 0.1627 | LR 0.000060\n",
      "Epoch 8 | Step 800/3657 | Loss 0.1631 | LR 0.000059\n",
      "Epoch 8 | Step 850/3657 | Loss 0.1640 | LR 0.000059\n",
      "Epoch 8 | Step 900/3657 | Loss 0.1653 | LR 0.000058\n",
      "Epoch 8 | Step 950/3657 | Loss 0.1656 | LR 0.000057\n",
      "Epoch 8 | Step 1000/3657 | Loss 0.1663 | LR 0.000057\n",
      "Epoch 8 | Step 1050/3657 | Loss 0.1654 | LR 0.000056\n",
      "Epoch 8 | Step 1100/3657 | Loss 0.1669 | LR 0.000056\n",
      "Epoch 8 | Step 1150/3657 | Loss 0.1669 | LR 0.000055\n",
      "Epoch 8 | Step 1200/3657 | Loss 0.1663 | LR 0.000055\n",
      "Epoch 8 | Step 1250/3657 | Loss 0.1666 | LR 0.000054\n",
      "Epoch 8 | Step 1300/3657 | Loss 0.1669 | LR 0.000054\n",
      "Epoch 8 | Step 1350/3657 | Loss 0.1668 | LR 0.000053\n",
      "Epoch 8 | Step 1400/3657 | Loss 0.1670 | LR 0.000053\n",
      "Epoch 8 | Step 1450/3657 | Loss 0.1662 | LR 0.000052\n",
      "Epoch 8 | Step 1500/3657 | Loss 0.1666 | LR 0.000052\n",
      "Epoch 8 | Step 1550/3657 | Loss 0.1668 | LR 0.000051\n",
      "Epoch 8 | Step 1600/3657 | Loss 0.1665 | LR 0.000051\n",
      "Epoch 8 | Step 1650/3657 | Loss 0.1654 | LR 0.000050\n",
      "Epoch 8 | Step 1700/3657 | Loss 0.1659 | LR 0.000050\n",
      "Epoch 8 | Step 1750/3657 | Loss 0.1656 | LR 0.000049\n",
      "Epoch 8 | Step 1800/3657 | Loss 0.1656 | LR 0.000049\n",
      "Epoch 8 | Step 1850/3657 | Loss 0.1655 | LR 0.000048\n",
      "Epoch 8 | Step 1900/3657 | Loss 0.1655 | LR 0.000048\n",
      "Epoch 8 | Step 1950/3657 | Loss 0.1659 | LR 0.000047\n",
      "Epoch 8 | Step 2000/3657 | Loss 0.1656 | LR 0.000047\n",
      "Epoch 8 | Step 2050/3657 | Loss 0.1656 | LR 0.000046\n",
      "Epoch 8 | Step 2100/3657 | Loss 0.1659 | LR 0.000046\n",
      "Epoch 8 | Step 2150/3657 | Loss 0.1656 | LR 0.000045\n",
      "Epoch 8 | Step 2200/3657 | Loss 0.1654 | LR 0.000045\n",
      "Epoch 8 | Step 2250/3657 | Loss 0.1660 | LR 0.000044\n",
      "Epoch 8 | Step 2300/3657 | Loss 0.1654 | LR 0.000044\n",
      "Epoch 8 | Step 2350/3657 | Loss 0.1657 | LR 0.000043\n",
      "Epoch 8 | Step 2400/3657 | Loss 0.1658 | LR 0.000043\n",
      "Epoch 8 | Step 2450/3657 | Loss 0.1659 | LR 0.000042\n",
      "Epoch 8 | Step 2500/3657 | Loss 0.1661 | LR 0.000042\n",
      "Epoch 8 | Step 2550/3657 | Loss 0.1662 | LR 0.000041\n",
      "Epoch 8 | Step 2600/3657 | Loss 0.1664 | LR 0.000041\n",
      "Epoch 8 | Step 2650/3657 | Loss 0.1662 | LR 0.000040\n",
      "Epoch 8 | Step 2700/3657 | Loss 0.1658 | LR 0.000040\n",
      "Epoch 8 | Step 2750/3657 | Loss 0.1660 | LR 0.000040\n",
      "Epoch 8 | Step 2800/3657 | Loss 0.1662 | LR 0.000039\n",
      "Epoch 8 | Step 2850/3657 | Loss 0.1665 | LR 0.000039\n",
      "Epoch 8 | Step 2900/3657 | Loss 0.1664 | LR 0.000038\n",
      "Epoch 8 | Step 2950/3657 | Loss 0.1671 | LR 0.000038\n",
      "Epoch 8 | Step 3000/3657 | Loss 0.1672 | LR 0.000037\n",
      "Epoch 8 | Step 3050/3657 | Loss 0.1671 | LR 0.000037\n",
      "Epoch 8 | Step 3100/3657 | Loss 0.1673 | LR 0.000036\n",
      "Epoch 8 | Step 3150/3657 | Loss 0.1677 | LR 0.000036\n",
      "Epoch 8 | Step 3200/3657 | Loss 0.1678 | LR 0.000036\n",
      "Epoch 8 | Step 3250/3657 | Loss 0.1681 | LR 0.000035\n",
      "Epoch 8 | Step 3300/3657 | Loss 0.1682 | LR 0.000035\n",
      "Epoch 8 | Step 3350/3657 | Loss 0.1685 | LR 0.000034\n",
      "Epoch 8 | Step 3400/3657 | Loss 0.1686 | LR 0.000034\n",
      "Epoch 8 | Step 3450/3657 | Loss 0.1686 | LR 0.000033\n",
      "Epoch 8 | Step 3500/3657 | Loss 0.1688 | LR 0.000033\n",
      "Epoch 8 | Step 3550/3657 | Loss 0.1687 | LR 0.000033\n",
      "Epoch 8 | Step 3600/3657 | Loss 0.1689 | LR 0.000032\n",
      "Epoch 8 | Step 3650/3657 | Loss 0.1690 | LR 0.000032\n",
      "Epoch 8 | Step 3657/3657 | Loss 0.1690 | LR 0.000032\n",
      "Epoch 8 done -> Val Loss: 0.2559 | Val Acc: 92.08%\n",
      "Epoch 9 | Step 50/3657 | Loss 0.1720 | LR 0.000031\n",
      "Epoch 9 | Step 100/3657 | Loss 0.1685 | LR 0.000031\n",
      "Epoch 9 | Step 150/3657 | Loss 0.1717 | LR 0.000030\n",
      "Epoch 9 | Step 200/3657 | Loss 0.1663 | LR 0.000030\n",
      "Epoch 9 | Step 250/3657 | Loss 0.1671 | LR 0.000030\n",
      "Epoch 9 | Step 300/3657 | Loss 0.1664 | LR 0.000029\n",
      "Epoch 9 | Step 350/3657 | Loss 0.1662 | LR 0.000029\n",
      "Epoch 9 | Step 400/3657 | Loss 0.1625 | LR 0.000028\n",
      "Epoch 9 | Step 450/3657 | Loss 0.1609 | LR 0.000028\n",
      "Epoch 9 | Step 500/3657 | Loss 0.1614 | LR 0.000028\n",
      "Epoch 9 | Step 550/3657 | Loss 0.1635 | LR 0.000027\n",
      "Epoch 9 | Step 600/3657 | Loss 0.1646 | LR 0.000027\n",
      "Epoch 9 | Step 650/3657 | Loss 0.1640 | LR 0.000026\n",
      "Epoch 9 | Step 700/3657 | Loss 0.1646 | LR 0.000026\n",
      "Epoch 9 | Step 750/3657 | Loss 0.1643 | LR 0.000026\n",
      "Epoch 9 | Step 800/3657 | Loss 0.1657 | LR 0.000025\n",
      "Epoch 9 | Step 850/3657 | Loss 0.1667 | LR 0.000025\n",
      "Epoch 9 | Step 900/3657 | Loss 0.1665 | LR 0.000025\n",
      "Epoch 9 | Step 950/3657 | Loss 0.1670 | LR 0.000024\n",
      "Epoch 9 | Step 1000/3657 | Loss 0.1667 | LR 0.000024\n",
      "Epoch 9 | Step 1050/3657 | Loss 0.1659 | LR 0.000023\n",
      "Epoch 9 | Step 1100/3657 | Loss 0.1661 | LR 0.000023\n",
      "Epoch 9 | Step 1150/3657 | Loss 0.1656 | LR 0.000023\n",
      "Epoch 9 | Step 1200/3657 | Loss 0.1655 | LR 0.000022\n",
      "Epoch 9 | Step 1250/3657 | Loss 0.1660 | LR 0.000022\n",
      "Epoch 9 | Step 1300/3657 | Loss 0.1664 | LR 0.000022\n",
      "Epoch 9 | Step 1350/3657 | Loss 0.1660 | LR 0.000021\n",
      "Epoch 9 | Step 1400/3657 | Loss 0.1652 | LR 0.000021\n",
      "Epoch 9 | Step 1450/3657 | Loss 0.1659 | LR 0.000021\n",
      "Epoch 9 | Step 1500/3657 | Loss 0.1665 | LR 0.000020\n",
      "Epoch 9 | Step 1550/3657 | Loss 0.1666 | LR 0.000020\n",
      "Epoch 9 | Step 1600/3657 | Loss 0.1670 | LR 0.000020\n",
      "Epoch 9 | Step 1650/3657 | Loss 0.1665 | LR 0.000019\n",
      "Epoch 9 | Step 1700/3657 | Loss 0.1662 | LR 0.000019\n",
      "Epoch 9 | Step 1750/3657 | Loss 0.1664 | LR 0.000019\n",
      "Epoch 9 | Step 1800/3657 | Loss 0.1663 | LR 0.000018\n",
      "Epoch 9 | Step 1850/3657 | Loss 0.1667 | LR 0.000018\n",
      "Epoch 9 | Step 1900/3657 | Loss 0.1663 | LR 0.000018\n",
      "Epoch 9 | Step 1950/3657 | Loss 0.1660 | LR 0.000017\n",
      "Epoch 9 | Step 2000/3657 | Loss 0.1656 | LR 0.000017\n",
      "Epoch 9 | Step 2050/3657 | Loss 0.1654 | LR 0.000017\n",
      "Epoch 9 | Step 2100/3657 | Loss 0.1657 | LR 0.000016\n",
      "Epoch 9 | Step 2150/3657 | Loss 0.1653 | LR 0.000016\n",
      "Epoch 9 | Step 2200/3657 | Loss 0.1649 | LR 0.000016\n",
      "Epoch 9 | Step 2250/3657 | Loss 0.1646 | LR 0.000015\n",
      "Epoch 9 | Step 2300/3657 | Loss 0.1647 | LR 0.000015\n",
      "Epoch 9 | Step 2350/3657 | Loss 0.1642 | LR 0.000015\n",
      "Epoch 9 | Step 2400/3657 | Loss 0.1643 | LR 0.000015\n",
      "Epoch 9 | Step 2450/3657 | Loss 0.1639 | LR 0.000014\n",
      "Epoch 9 | Step 2500/3657 | Loss 0.1637 | LR 0.000014\n",
      "Epoch 9 | Step 2550/3657 | Loss 0.1639 | LR 0.000014\n",
      "Epoch 9 | Step 2600/3657 | Loss 0.1643 | LR 0.000013\n",
      "Epoch 9 | Step 2650/3657 | Loss 0.1645 | LR 0.000013\n",
      "Epoch 9 | Step 2700/3657 | Loss 0.1643 | LR 0.000013\n",
      "Epoch 9 | Step 2750/3657 | Loss 0.1644 | LR 0.000013\n",
      "Epoch 9 | Step 2800/3657 | Loss 0.1646 | LR 0.000012\n",
      "Epoch 9 | Step 2850/3657 | Loss 0.1645 | LR 0.000012\n",
      "Epoch 9 | Step 2900/3657 | Loss 0.1644 | LR 0.000012\n",
      "Epoch 9 | Step 2950/3657 | Loss 0.1645 | LR 0.000012\n",
      "Epoch 9 | Step 3000/3657 | Loss 0.1649 | LR 0.000011\n",
      "Epoch 9 | Step 3050/3657 | Loss 0.1649 | LR 0.000011\n",
      "Epoch 9 | Step 3100/3657 | Loss 0.1650 | LR 0.000011\n",
      "Epoch 9 | Step 3150/3657 | Loss 0.1642 | LR 0.000011\n",
      "Epoch 9 | Step 3200/3657 | Loss 0.1644 | LR 0.000010\n",
      "Epoch 9 | Step 3250/3657 | Loss 0.1647 | LR 0.000010\n",
      "Epoch 9 | Step 3300/3657 | Loss 0.1645 | LR 0.000010\n",
      "Epoch 9 | Step 3350/3657 | Loss 0.1647 | LR 0.000010\n",
      "Epoch 9 | Step 3400/3657 | Loss 0.1647 | LR 0.000009\n",
      "Epoch 9 | Step 3450/3657 | Loss 0.1646 | LR 0.000009\n",
      "Epoch 9 | Step 3500/3657 | Loss 0.1650 | LR 0.000009\n",
      "Epoch 9 | Step 3550/3657 | Loss 0.1649 | LR 0.000009\n",
      "Epoch 9 | Step 3600/3657 | Loss 0.1652 | LR 0.000008\n",
      "Epoch 9 | Step 3650/3657 | Loss 0.1651 | LR 0.000008\n",
      "Epoch 9 | Step 3657/3657 | Loss 0.1649 | LR 0.000008\n",
      "Epoch 9 done -> Val Loss: 0.2550 | Val Acc: 92.07%\n",
      "Epoch 10 | Step 50/3657 | Loss 0.1534 | LR 0.000008\n",
      "Epoch 10 | Step 100/3657 | Loss 0.1621 | LR 0.000008\n",
      "Epoch 10 | Step 150/3657 | Loss 0.1607 | LR 0.000007\n",
      "Epoch 10 | Step 200/3657 | Loss 0.1612 | LR 0.000007\n",
      "Epoch 10 | Step 250/3657 | Loss 0.1648 | LR 0.000007\n",
      "Epoch 10 | Step 300/3657 | Loss 0.1611 | LR 0.000007\n",
      "Epoch 10 | Step 350/3657 | Loss 0.1603 | LR 0.000007\n",
      "Epoch 10 | Step 400/3657 | Loss 0.1613 | LR 0.000006\n",
      "Epoch 10 | Step 450/3657 | Loss 0.1626 | LR 0.000006\n",
      "Epoch 10 | Step 500/3657 | Loss 0.1605 | LR 0.000006\n",
      "Epoch 10 | Step 550/3657 | Loss 0.1584 | LR 0.000006\n",
      "Epoch 10 | Step 600/3657 | Loss 0.1617 | LR 0.000006\n",
      "Epoch 10 | Step 650/3657 | Loss 0.1611 | LR 0.000006\n",
      "Epoch 10 | Step 700/3657 | Loss 0.1611 | LR 0.000005\n",
      "Epoch 10 | Step 750/3657 | Loss 0.1624 | LR 0.000005\n",
      "Epoch 10 | Step 800/3657 | Loss 0.1605 | LR 0.000005\n",
      "Epoch 10 | Step 850/3657 | Loss 0.1606 | LR 0.000005\n",
      "Epoch 10 | Step 900/3657 | Loss 0.1608 | LR 0.000005\n",
      "Epoch 10 | Step 950/3657 | Loss 0.1619 | LR 0.000004\n",
      "Epoch 10 | Step 1000/3657 | Loss 0.1622 | LR 0.000004\n",
      "Epoch 10 | Step 1050/3657 | Loss 0.1626 | LR 0.000004\n",
      "Epoch 10 | Step 1100/3657 | Loss 0.1622 | LR 0.000004\n",
      "Epoch 10 | Step 1150/3657 | Loss 0.1620 | LR 0.000004\n",
      "Epoch 10 | Step 1200/3657 | Loss 0.1616 | LR 0.000004\n",
      "Epoch 10 | Step 1250/3657 | Loss 0.1614 | LR 0.000004\n",
      "Epoch 10 | Step 1300/3657 | Loss 0.1620 | LR 0.000003\n",
      "Epoch 10 | Step 1350/3657 | Loss 0.1604 | LR 0.000003\n",
      "Epoch 10 | Step 1400/3657 | Loss 0.1611 | LR 0.000003\n",
      "Epoch 10 | Step 1450/3657 | Loss 0.1610 | LR 0.000003\n",
      "Epoch 10 | Step 1500/3657 | Loss 0.1617 | LR 0.000003\n",
      "Epoch 10 | Step 1550/3657 | Loss 0.1620 | LR 0.000003\n",
      "Epoch 10 | Step 1600/3657 | Loss 0.1620 | LR 0.000003\n",
      "Epoch 10 | Step 1650/3657 | Loss 0.1624 | LR 0.000002\n",
      "Epoch 10 | Step 1700/3657 | Loss 0.1626 | LR 0.000002\n",
      "Epoch 10 | Step 1750/3657 | Loss 0.1628 | LR 0.000002\n",
      "Epoch 10 | Step 1800/3657 | Loss 0.1624 | LR 0.000002\n",
      "Epoch 10 | Step 1850/3657 | Loss 0.1624 | LR 0.000002\n",
      "Epoch 10 | Step 1900/3657 | Loss 0.1628 | LR 0.000002\n",
      "Epoch 10 | Step 1950/3657 | Loss 0.1629 | LR 0.000002\n",
      "Epoch 10 | Step 2000/3657 | Loss 0.1631 | LR 0.000002\n",
      "Epoch 10 | Step 2050/3657 | Loss 0.1633 | LR 0.000002\n",
      "Epoch 10 | Step 2100/3657 | Loss 0.1634 | LR 0.000001\n",
      "Epoch 10 | Step 2150/3657 | Loss 0.1634 | LR 0.000001\n",
      "Epoch 10 | Step 2200/3657 | Loss 0.1633 | LR 0.000001\n",
      "Epoch 10 | Step 2250/3657 | Loss 0.1631 | LR 0.000001\n",
      "Epoch 10 | Step 2300/3657 | Loss 0.1628 | LR 0.000001\n",
      "Epoch 10 | Step 2350/3657 | Loss 0.1630 | LR 0.000001\n",
      "Epoch 10 | Step 2400/3657 | Loss 0.1633 | LR 0.000001\n",
      "Epoch 10 | Step 2450/3657 | Loss 0.1631 | LR 0.000001\n",
      "Epoch 10 | Step 2500/3657 | Loss 0.1628 | LR 0.000001\n",
      "Epoch 10 | Step 2550/3657 | Loss 0.1626 | LR 0.000001\n",
      "Epoch 10 | Step 2600/3657 | Loss 0.1628 | LR 0.000001\n",
      "Epoch 10 | Step 2650/3657 | Loss 0.1629 | LR 0.000001\n",
      "Epoch 10 | Step 2700/3657 | Loss 0.1629 | LR 0.000001\n",
      "Epoch 10 | Step 2750/3657 | Loss 0.1633 | LR 0.000001\n",
      "Epoch 10 | Step 2800/3657 | Loss 0.1634 | LR 0.000000\n",
      "Epoch 10 | Step 2850/3657 | Loss 0.1635 | LR 0.000000\n",
      "Epoch 10 | Step 2900/3657 | Loss 0.1631 | LR 0.000000\n",
      "Epoch 10 | Step 2950/3657 | Loss 0.1634 | LR 0.000000\n",
      "Epoch 10 | Step 3000/3657 | Loss 0.1629 | LR 0.000000\n",
      "Epoch 10 | Step 3050/3657 | Loss 0.1629 | LR 0.000000\n",
      "Epoch 10 | Step 3100/3657 | Loss 0.1629 | LR 0.000000\n",
      "Epoch 10 | Step 3150/3657 | Loss 0.1628 | LR 0.000000\n",
      "Epoch 10 | Step 3200/3657 | Loss 0.1628 | LR 0.000000\n",
      "Epoch 10 | Step 3250/3657 | Loss 0.1625 | LR 0.000000\n",
      "Epoch 10 | Step 3300/3657 | Loss 0.1627 | LR 0.000000\n",
      "Epoch 10 | Step 3350/3657 | Loss 0.1622 | LR 0.000000\n",
      "Epoch 10 | Step 3400/3657 | Loss 0.1626 | LR 0.000000\n",
      "Epoch 10 | Step 3450/3657 | Loss 0.1628 | LR 0.000000\n",
      "Epoch 10 | Step 3500/3657 | Loss 0.1629 | LR 0.000000\n",
      "Epoch 10 | Step 3550/3657 | Loss 0.1631 | LR 0.000000\n",
      "Epoch 10 | Step 3600/3657 | Loss 0.1630 | LR 0.000000\n",
      "Epoch 10 | Step 3650/3657 | Loss 0.1630 | LR 0.000000\n",
      "Epoch 10 | Step 3657/3657 | Loss 0.1629 | LR 0.000000\n",
      "Epoch 10 done -> Val Loss: 0.2553 | Val Acc: 92.07%\n",
      "Training complete.\n",
      "Best Val Acc: 92.19%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Model: head-only fine-tuning\n",
    "# -----------------------------\n",
    "print(\"Loading model…\")\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_classes,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,  # replaces the classification head if shape differs\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Freeze everything except the classifier head\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Verify which params train\n",
    "trainable_params = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(\"Trainable params:\", trainable_params)\n",
    "\n",
    "# Optimizer/scheduler\n",
    "head_params = (p for p in model.classifier.parameters() if p.requires_grad)\n",
    "optimizer = torch.optim.AdamW(head_params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / GRAD_ACCUM_STEPS)\n",
    "max_train_steps = EPOCHS * num_update_steps_per_epoch\n",
    "num_warmup_steps = int(WARMUP_RATIO * max_train_steps)\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=max_train_steps\n",
    ")\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "# -----------------------------\n",
    "# Training / Evaluation loops\n",
    "# -----------------------------\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total, loss_sum = 0, 0, 0.0\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"])\n",
    "            loss = loss_fn(outputs.logits, batch[\"labels\"])\n",
    "            loss_sum += loss.item() * batch[\"labels\"].size(0)\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            correct += (preds == batch[\"labels\"]).sum().item()\n",
    "            total += batch[\"labels\"].size(0)\n",
    "    avg_loss = loss_sum / max(1, total)\n",
    "    acc = correct / max(1, total)\n",
    "    return avg_loss, acc\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "best_val_acc = 0.0\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for step, batch in enumerate(train_loader, start=1):\n",
    "        batch = {k: v.to(device, non_blocking=True) for k, v in batch.items()}\n",
    "        with torch.cuda.amp.autocast(enabled=AMP):\n",
    "            outputs = model(pixel_values=batch[\"pixel_values\"], labels=batch[\"labels\"])\n",
    "            loss = outputs.loss / GRAD_ACCUM_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if step % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            lr_scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "        running_loss += loss.item() * GRAD_ACCUM_STEPS\n",
    "\n",
    "        if step % 50 == 0 or step == len(train_loader):\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            print(f\"Epoch {epoch} | Step {step}/{len(train_loader)} | \"\n",
    "                  f\"Loss {running_loss/step:.4f} | LR {current_lr:.6f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    val_loss, val_acc = evaluate(model, val_loader)\n",
    "    print(f\"Epoch {epoch} done -> Val Loss: {val_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # Save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        save_path = os.path.join(OUTPUT_DIR, \"best\")\n",
    "        model.save_pretrained(save_path)\n",
    "        processor.save_pretrained(save_path)\n",
    "        # Also save just the head weights if you want:\n",
    "        torch.save(model.classifier.state_dict(), os.path.join(OUTPUT_DIR, \"classifier_head.pt\"))\n",
    "        print(f\"New best ({best_val_acc*100:.2f}%). Saved to {save_path}\")\n",
    "\n",
    "# Final save\n",
    "model.save_pretrained(os.path.join(OUTPUT_DIR, \"last\"))\n",
    "processor.save_pretrained(os.path.join(OUTPUT_DIR, \"last\"))\n",
    "\n",
    "print(\"Training complete.\")\n",
    "print(f\"Best Val Acc: {best_val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60632a-e890-4e7c-962e-c4e4baafa947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
