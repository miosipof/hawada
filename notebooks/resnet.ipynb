{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77bb846",
   "metadata": {},
   "source": [
    "# Example: optimize ResNet-18 for a target GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3acd043-f2be-4629-91da-6ea5d1e1668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio -U\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "105537a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "sys.path.append(str(pathlib.Path(\"resnet.ipynb\").resolve().parents[1]))\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "03059f52-87d3-42eb-be83-b9ecfce45a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kagglehub\n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"lyfora/processed-imagenet-dataset-224\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3476b93-d9e8-43c9-94ba-fa5b89ed7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"/data/imagenet100-224/train\", exist_ok=True)\n",
    "# !mv /root/.cache/kagglehub/datasets/lyfora/processed-imagenet-dataset-224/versions/1 /data/imagenet100-224/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2c5e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: resnet18\n",
      "Dataset root: /data/imagenet100-224/train\n"
     ]
    }
   ],
   "source": [
    "# Load / batch helpers\n",
    "from core.utils import load_yaml, _images_from_batch\n",
    "\n",
    "# Script to build config from a recipe\n",
    "from examples.run_resnet_optimize import build_from_recipe\n",
    "\n",
    "# Get needed metadata from recipe and download the base model\n",
    "pack = build_from_recipe(\"../recipes/RTX4090/resnet18_imagenet224.yaml\")\n",
    "\n",
    "print(\"Model name:\", pack['recipe']['model']['name'])\n",
    "print(\"Dataset root:\", pack['recipe']['data']['train_root'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b31d92-a452-47bc-b542-17cd7b2d6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download snaphots: gated model + slim (pruned) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17ed55d8-f304-4cdb-9b06-9cc9a3a4a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd54992-e0bb-451a-bb52-c93644e2d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_model_repo  = \"hawada/resnet18-rtx4090-gated\"\n",
    "gated_local_dir   = \"ckpt/resnet/gated\"\n",
    "\n",
    "# Download pre-trained gated model (full weights + tuned gates for pruning)\n",
    "# snapshot_download(repo_id=gated_model_repo, local_dir=gated_local_dir, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2c6022-0c4a-41d9-9a88-516852941d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model_repo  = \"hawada/resnet18-rtx4090-slim\"\n",
    "slim_local_dir   = \"ckpt/resnet/slim\"\n",
    "\n",
    "# Download pre-trained slim model (already pruned)\n",
    "# snapshot_download(repo_id=slim_model_repo, local_dir=slim_local_dir, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d336c0af-fc16-4c58-93e7-fb10c00d478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.ckpt.resnet.slim.minimal_resnet_loader import load_student\n",
    "\n",
    "# Load slim model using a custom loader\n",
    "slim_model  = load_student(slim_local_dir+\"/pytorch_model.bin\", device=DEVICE)\n",
    "\n",
    "# Load gateg model using a custom loader\n",
    "gated_model = load_student(gated_local_dir+\"/pytorch_model.bin\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0179923c-7292-4409-950c-9294334bdb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmarking with batch size = 512...\n",
      "\n",
      "Base: mean=92.094ms p95=92.516ms\n",
      "Slim: mean=70.768ms p95=70.941ms\n",
      "\n",
      "Speedup=23.16%\n"
     ]
    }
   ],
   "source": [
    "from adapters.torchvision.resnet import ResNetAdapter\n",
    "from core.profiler import measure_latency_ms\n",
    "\n",
    "B = pack[\"batch_size\"]; H = W = pack[\"img_size\"]\n",
    "\n",
    "print(f\"Starting benchmarking with batch size = {B}...\\n\")\n",
    "mean_keep, p95_keep, _ = measure_latency_ms(ResNetAdapter.export_keepall(gated_model), (B, 3, H, W), device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim_model, (B, 3, H, W), device=DEVICE)\n",
    "\n",
    "print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "if mean_keep > 0:\n",
    "    print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9a282-d9d2-4f07-98b3-415c7fae8692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd2730-9ee0-45cc-bab1-90207d6c9962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06de90e-9b90-4439-afbe-57bf871d39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prune gated model with a custom export policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ba03032-527b-48a6-bbb1-dc2bbcda315c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmarking with batch size = 512...\n",
      "\n",
      "Base: mean=91.822ms p95=92.367ms\n",
      "Slim: mean=53.908ms p95=54.082ms\n",
      "\n",
      "Speedup=41.29%\n"
     ]
    }
   ],
   "source": [
    "from adapters.torchvision.resnet import ResNetExportPolicy\n",
    "from core.export import Rounding as CoreRounding\n",
    "\n",
    "M = 2    # Multiples of this number will be used for pruned layers shapes\n",
    "K = 0.1  # Minimum ratio of kept shapes\n",
    "\n",
    "policy = ResNetExportPolicy(\n",
    "    warmup_steps=0,\n",
    "    rounding=CoreRounding(floor_groups=1, multiple_groups=M, min_keep_ratio=K),\n",
    "    min_keep_ratio=K,\n",
    ")\n",
    "\n",
    "# Obtain a new pruned model\n",
    "slim_model_new = ResNetAdapter.export_pruned(gated_model, policy, step=9999).to(DEVICE)\n",
    "\n",
    "print(f\"Starting benchmarking with batch size = {B}...\\n\")\n",
    "mean_keep, p95_keep, _ = measure_latency_ms(ResNetAdapter.export_keepall(gated_model), (B, 3, H, W), device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim_model_new, (B, 3, H, W), device=DEVICE)\n",
    "\n",
    "print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "if mean_keep > 0:\n",
    "    print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e129b0-4f43-438c-bfd4-ede0668c7484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf8507-0795-4c4b-af87-1c33a28b8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tune a new slim model (distillation from teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94740a-dfc1-4b71-9288-2cde9f074e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.finetune import FinetuneConfig, finetune_student, recalibrate_bn_stats\n",
    "from core.distill import KDConfig\n",
    "\n",
    "# Get the suggested fine-tuning config from the recipe package\n",
    "\n",
    "teacher = pack[\"teacher\"] # Distallation from a bigger ResNet by default\n",
    "ft_epochs = 1 # int(pack[\"recipe\"].get(\"finetune\", {}).get(\"epochs\", 10))\n",
    "learning_rate = float(pack[\"recipe\"].get(\"finetune\", {}).get(\"lr\", 3e-4))\n",
    "weight_decay = float(pack[\"recipe\"].get(\"finetune\", {}).get(\"wd\", 1e-5))\n",
    "\n",
    "train_loader = pack[\"train_loader\"]\n",
    "val_loader   = pack[\"val_loader\"]\n",
    "\n",
    "# Build a config for fine-tuning\n",
    "ft_cfg = FinetuneConfig(\n",
    "    epochs=ft_epochs,\n",
    "    lr=learning_rate,\n",
    "    wd=weight_decay,\n",
    "    kd=KDConfig(**pack[\"recipe\"].get(\"trainer\", {}).get(\"kd\", {})),\n",
    "    amp=bool(pack[\"recipe\"].get(\"trainer\", {}).get(\"amp\", True)),\n",
    "    mse_weight = float(pack[\"recipe\"].get(\"trainer\", {}).get(\"mse_weight\", 0.0)),\n",
    "    device=DEVICE,\n",
    "    log_every=50,\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting fine tuning for {ft_epochs} epochs, LR={learning_rate} ...\\n\")\n",
    "slim_finetuned = finetune_student(\n",
    "    slim_model_new,\n",
    "    teacher,\n",
    "    train_loader,\n",
    "    get_student_logits=lambda m, batch: m(_images_from_batch(batch)),\n",
    "    get_teacher_logits=lambda m, batch: m(_images_from_batch(batch)).detach(),\n",
    "    cfg=ft_cfg,\n",
    "    val_loader=val_loader,\n",
    ")\n",
    "\n",
    "# Recalibrate BatchNorm stats before export\n",
    "recalibrate_bn_stats(slim_finetuned, train_loader, max_batches=1000)\n",
    "\n",
    "# # Now you have a faster model that behaves well on the selected dataset\n",
    "# import torch\n",
    "# out_path = \"resnet18_slim_new.pth\"\n",
    "# torch.save(slim_finetuned.state_dict(), out_path)\n",
    "# print(\"Saved pruned model to\", out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae006753-505f-4f1d-b4be-7e29745c0bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43940d3e-e7b1-4969-9566-5caf8f470341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90baaaab-355b-44c7-9651-c19b016a8f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hawada)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
