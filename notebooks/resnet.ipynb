{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77bb846",
   "metadata": {},
   "source": [
    "# Example: optimize ResNet-18 for a target GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3acd043-f2be-4629-91da-6ea5d1e1668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio -U\n",
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "105537a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "sys.path.append(str(pathlib.Path(\"resnet.ipynb\").resolve().parents[1]))\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03059f52-87d3-42eb-be83-b9ecfce45a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kagglehub\n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"lyfora/processed-imagenet-dataset-224\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3476b93-d9e8-43c9-94ba-fa5b89ed7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"/data/imagenet100-224/train\", exist_ok=True)\n",
    "# !mv /root/.cache/kagglehub/datasets/lyfora/processed-imagenet-dataset-224/versions/1 /data/imagenet100-224/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2c5e4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: resnet18\n",
      "Dataset root: /data/imagenet100-224/train\n"
     ]
    }
   ],
   "source": [
    "# Load / batch helpers\n",
    "from core.utils import load_yaml, _images_from_batch\n",
    "\n",
    "# Script to build config from a recipe\n",
    "from examples.run_resnet_optimize import build_from_recipe\n",
    "\n",
    "# Get needed metadata from recipe and download the base model\n",
    "pack = build_from_recipe(\"../recipes/RTX4090/resnet18_imagenet224.yaml\")\n",
    "\n",
    "print(\"Model name:\", pack['recipe']['model']['name'])\n",
    "print(\"Dataset root:\", pack['recipe']['data']['train_root'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b31d92-a452-47bc-b542-17cd7b2d6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download snaphots: gated model + slim (pruned) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ed55d8-f304-4cdb-9b06-9cc9a3a4a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/hawada/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import transformers\n",
    "# from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd54992-e0bb-451a-bb52-c93644e2d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_model_repo  = \"hawada/resnet18-rtx4090-gated\"\n",
    "gated_local_dir   = \"ckpt/resnet/gated\"\n",
    "\n",
    "# Download pre-trained gated model (full weights + tuned gates for pruning)\n",
    "# snapshot_download(repo_id=gated_model_repo, local_dir=gated_local_dir, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2c6022-0c4a-41d9-9a88-516852941d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model_repo  = \"hawada/resnet18-rtx4090-slim\"\n",
    "slim_local_dir   = \"ckpt/resnet/slim\"\n",
    "\n",
    "# Download pre-trained slim model (already pruned)\n",
    "# snapshot_download(repo_id=slim_model_repo, local_dir=slim_local_dir, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d336c0af-fc16-4c58-93e7-fb10c00d478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.ckpt.resnet.slim.minimal_resnet_loader import load_student\n",
    "\n",
    "# Load slim model using a custom loader\n",
    "slim_model  = load_student(slim_local_dir+\"/pytorch_model.bin\", device=DEVICE)\n",
    "\n",
    "# Load gateg model using a custom loader\n",
    "gated_model = load_student(gated_local_dir+\"/pytorch_model.bin\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0179923c-7292-4409-950c-9294334bdb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters.torchvision.resnet import ResNetAdapter\n",
    "from core.profiler import measure_latency_ms\n",
    "\n",
    "# B = pack[\"batch_size\"]; H = W = pack[\"img_size\"]\n",
    "\n",
    "# print(f\"Starting benchmarking with batch size = {B}...\\n\")\n",
    "# mean_keep, p95_keep, _ = measure_latency_ms(ResNetAdapter.export_keepall(gated_model), (B, 3, H, W), device=DEVICE)\n",
    "# mean_slim, p95_slim, _ = measure_latency_ms(slim_model, (B, 3, H, W), device=DEVICE)\n",
    "\n",
    "# print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "# print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "# if mean_keep > 0:\n",
    "#     print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a9a282-d9d2-4f07-98b3-415c7fae8692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfd2730-9ee0-45cc-bab1-90207d6c9962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06de90e-9b90-4439-afbe-57bf871d39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prune gated model with a custom export policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ba03032-527b-48a6-bbb1-dc2bbcda315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters.torchvision.resnet import ResNetExportPolicy\n",
    "from core.export import Rounding as CoreRounding\n",
    "\n",
    "# M = 2    # Multiples of this number will be used for pruned layers shapes\n",
    "# K = 0.1  # Minimum ratio of kept shapes\n",
    "\n",
    "# policy = ResNetExportPolicy(\n",
    "#     warmup_steps=0,\n",
    "#     rounding=CoreRounding(floor_groups=1, multiple_groups=M, min_keep_ratio=K),\n",
    "#     min_keep_ratio=K,\n",
    "# )\n",
    "\n",
    "# # Obtain a new pruned model\n",
    "# slim_model_new = ResNetAdapter.export_pruned(gated_model, policy, step=9999).to(DEVICE)\n",
    "\n",
    "# print(f\"Starting benchmarking with batch size = {B}...\\n\")\n",
    "# mean_keep, p95_keep, _ = measure_latency_ms(ResNetAdapter.export_keepall(gated_model), (B, 3, H, W), device=DEVICE)\n",
    "# mean_slim, p95_slim, _ = measure_latency_ms(slim_model_new, (B, 3, H, W), device=DEVICE)\n",
    "\n",
    "# print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "# print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "# if mean_keep > 0:\n",
    "#     print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e129b0-4f43-438c-bfd4-ede0668c7484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0cf8507-0795-4c4b-af87-1c33a28b8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine-tune a new slim model (distillation from teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb94740a-dfc1-4b71-9288-2cde9f074e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from core.finetune import FinetuneConfig, finetune_student, recalibrate_bn_stats\n",
    "# from core.distill import KDConfig\n",
    "\n",
    "# # Get the suggested fine-tuning config from the recipe package\n",
    "\n",
    "# teacher = pack[\"teacher\"] # Distallation from a bigger ResNet by default\n",
    "# ft_epochs = 1 # int(pack[\"recipe\"].get(\"finetune\", {}).get(\"epochs\", 10))\n",
    "# learning_rate = float(pack[\"recipe\"].get(\"finetune\", {}).get(\"lr\", 3e-4))\n",
    "# weight_decay = float(pack[\"recipe\"].get(\"finetune\", {}).get(\"wd\", 1e-5))\n",
    "\n",
    "# train_loader = pack[\"train_loader\"]\n",
    "# val_loader   = pack[\"val_loader\"]\n",
    "\n",
    "# # Build a config for fine-tuning\n",
    "# ft_cfg = FinetuneConfig(\n",
    "#     epochs=ft_epochs,\n",
    "#     lr=learning_rate,\n",
    "#     wd=weight_decay,\n",
    "#     kd=KDConfig(**pack[\"recipe\"].get(\"trainer\", {}).get(\"kd\", {})),\n",
    "#     amp=bool(pack[\"recipe\"].get(\"trainer\", {}).get(\"amp\", True)),\n",
    "#     mse_weight = float(pack[\"recipe\"].get(\"trainer\", {}).get(\"mse_weight\", 0.0)),\n",
    "#     device=DEVICE,\n",
    "#     log_every=50,\n",
    "# )\n",
    "\n",
    "# print(f\"\\nStarting fine tuning for {ft_epochs} epochs, LR={learning_rate} ...\\n\")\n",
    "# slim_finetuned = finetune_student(\n",
    "#     slim_model_new,\n",
    "#     teacher,\n",
    "#     train_loader,\n",
    "#     get_student_logits=lambda m, batch: m(_images_from_batch(batch)),\n",
    "#     get_teacher_logits=lambda m, batch: m(_images_from_batch(batch)).detach(),\n",
    "#     cfg=ft_cfg,\n",
    "#     val_loader=val_loader,\n",
    "# )\n",
    "\n",
    "# # Recalibrate BatchNorm stats before export\n",
    "# recalibrate_bn_stats(slim_finetuned, train_loader, max_batches=1000)\n",
    "\n",
    "# # # Now you have a faster model that behaves well on the selected dataset\n",
    "# # import torch\n",
    "# # out_path = \"resnet18_slim_new.pth\"\n",
    "# # torch.save(slim_finetuned.state_dict(), out_path)\n",
    "# # print(\"Saved pruned model to\", out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae006753-505f-4f1d-b4be-7e29745c0bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab718bc4-02c3-4f33-9c5b-380983c4f35d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "532a735f-d6d6-40ac-92a5-b3d978251fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train a new gated model on your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb521f3-21e1-426f-95f5-d639e25c6410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31827c-1a5c-456f-a1c4-a316d52d439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del gated_model, slim_model, slim_model_new, slim_finetuned\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87bb1109-e949-4d64-a70c-d549b4a1fa04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning policy: multiple = 1\n",
      "Pruning policy: min keep ratio = 0.8\n",
      "\n",
      "Training conf: early stopping patience = 3\n",
      "Training conf: LR for gates = 0.01\n",
      "Training conf: LR for linear layers = 0.0001\n",
      "Training conf: LR for affine layers = 0.0003\n"
     ]
    }
   ],
   "source": [
    "from core.train import TrainerConfig\n",
    "\n",
    "\n",
    "os.makedirs(\"runs/resnet18\", exist_ok=True)\n",
    "\n",
    "# Student to prune and teracher to distill\n",
    "student = pack[\"student\"] # A new instance of ResNet-18\n",
    "teacher = pack[\"teacher\"] # A bigger ResNet-50\n",
    "\n",
    "# Loaders built via ImageNet224 dataset\n",
    "train_loader = pack[\"train_loader\"]\n",
    "val_loader   = pack[\"val_loader\"]\n",
    "\n",
    "# Differentiable proxy to estimate latency\n",
    "proxy = pack[\"proxy\"]\n",
    "\n",
    "# Pruning policy\n",
    "export_policy = pack[\"export_policy\"]\n",
    "\n",
    "# Training configuration\n",
    "train_cfg = pack[\"trainer_cfg\"]\n",
    "\n",
    "\n",
    "print(\"Pruning policy: multiple =\", export_policy.rounding.multiple_groups)\n",
    "print(\"Pruning policy: min keep ratio =\", export_policy.rounding.min_keep_ratio)\n",
    "\n",
    "print(\"\\nTraining conf: early stopping patience =\", train_cfg.early_stopping_patience)\n",
    "print(\"Training conf: LR for gates =\", train_cfg.lr_gate)\n",
    "print(\"Training conf: LR for linear layers =\", train_cfg.lr_linear)\n",
    "print(\"Training conf: LR for affine layers =\", train_cfg.lr_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea6f1b65-9d61-46e3-b7b2-7e7c7a23c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.train import LagrangeTrainer\n",
    "\n",
    "# Build training configuration from the recipe package\n",
    "trainer = LagrangeTrainer(\n",
    "    student=student,\n",
    "    teacher=teacher,\n",
    "    proxy=proxy,  \n",
    "    adapter_get_student_logits=lambda m, batch: m(_images_from_batch(batch)),\n",
    "    adapter_get_teacher_logits=lambda m, batch: m(_images_from_batch(batch)).detach(),\n",
    "    adapter_export_keepall=ResNetAdapter.export_keepall,\n",
    "    adapter_export_pruned=lambda m, pol, step: ResNetAdapter.export_pruned(m, pol, step),\n",
    "    export_policy=export_policy,\n",
    "    cfg=train_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f330d9-2cfb-456d-9a5e-d2ef46f59ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "lambdas = []\n",
    "for ep in range(EPOCHS):\n",
    "    print(f\"=== Epoch {ep+1}/{EPOCHS} ===\")\n",
    "    lam = trainer.train_epoch(train_loader)\n",
    "    lambdas.append(lam)\n",
    "\n",
    "    last = lambdas[:train_cfg.early_stopping_patience]\n",
    "    last = [x for x in last if x < train_cfg.early_stopping_lambda]\n",
    "    if len(last) == train_cfg.early_stopping_patience:\n",
    "        print(f\"Early stopping: lambda < {train_cfg.early_stopping_lambda} for last {train_cfg.early_stopping_patience} epochs\")\n",
    "        break\n",
    "\n",
    "\n",
    "# # Save gated model        \n",
    "# out_path = os.path.join(\"runs/resnet\", \"resnet18_gated.pth\")\n",
    "# torch.save(student.state_dict(), out_path)\n",
    "# print(\"Saved gated model to\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55684f1c-2074-4bee-ac96-1b0c0de5867c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e087c70-2e84-4b54-94bd-abe65932781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grid search the best pruning multiples on your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24837c6d-eaed-49e5-9f11-39191ccaeba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running export grid search...\n",
      "[0/4] multiple_groups=2 | mean_ms=70.899\n",
      "[1/4] multiple_groups=3 | mean_ms=70.962\n",
      "[2/4] multiple_groups=4 | mean_ms=71.049\n",
      "[3/4] multiple_groups=5 | mean_ms=71.202\n",
      "Best export params: {'multiple_groups': 2}\n",
      "Recalibrating BN stats on the slim model...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from examples.run_resnet_optimize import grid_search_export\n",
    "\n",
    "print(\"Running export grid search...\")\n",
    "\n",
    "# Multiples - set up accorging to your GPU or use recipe\n",
    "multiples = pack[\"recipe\"].get(\"export\").get(\"grid_multiple_groups\")\n",
    "\n",
    "res = grid_search_export(\n",
    "    student, # Gated model with pre-trained gate weights\n",
    "    device=DEVICE,\n",
    "    img_size=pack[\"img_size\"],\n",
    "    B=pack[\"batch_size\"],\n",
    "    multiples=multiples,\n",
    "    min_keep_ratio=float(pack[\"recipe\"].get(\"trainer\").get(\"lagrange\").get(\"min_keep_ratio\")),\n",
    ")\n",
    "\n",
    "\n",
    "# Select the fastest model\n",
    "slim = res[\"best_model\"]\n",
    "print(\"Best export params:\", res[\"best_params\"])\n",
    "\n",
    "# BatchNorm recalibration before saving the model\n",
    "print(\"\\nRecalibrating BN stats on the slim model...\")\n",
    "ResNetAdapter.bn_recalibration(slim, train_loader, num_batches=1000, device=DEVICE)\n",
    "print(\"Done\")\n",
    "\n",
    "# # Save model\n",
    "# out_path = os.path.join(\"runs/resnet\", \"resnet18_slim.pth\")\n",
    "# torch.save(slim, out_path)\n",
    "# print(\"Saved pruned model to\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43cc9f00-7fb8-434f-b440-296b451f5041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmarking with batch size = 512...\n",
      "\n",
      "Base: mean=93.156ms p95=93.591ms\n",
      "Slim: mean=71.559ms p95=71.835ms\n",
      "\n",
      "Speedup=23.18%\n"
     ]
    }
   ],
   "source": [
    "B = pack[\"batch_size\"]; H = W = pack[\"img_size\"]\n",
    "\n",
    "print(f\"Starting benchmarking with batch size = {B}...\\n\")\n",
    "mean_keep, p95_keep, _ = measure_latency_ms(ResNetAdapter.export_keepall(student), (B, 3, H, W), device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim, (B, 3, H, W), device=DEVICE)\n",
    "\n",
    "print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "if mean_keep > 0:\n",
    "    print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d9e55-ba1f-49b2-a54d-4cdce217f2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you have a slim model optimized for your GPU. \n",
    "# You can fine-tune it on your downstream task with a teacher, as we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e146b0-b2b8-4ed3-b1c2-a9a95d13ea25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fe476-d12b-4966-a04a-68c5d3623371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de6e938-92e4-4193-bdb8-3442096ab7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5fcbc-09eb-495b-81cc-4731740f8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m tools.export_to_hf \\ \n",
    "# --task resnet \\ \n",
    "# --base_id torchvision/resnet18 \\ \n",
    "# --student_ckpt runs/resnet18/resnet18_gated.pth \\ \n",
    "# --slim_ckpt runs/resnet18/resnet18_slim.pth \\ \n",
    "# --repo_gated hawada/resnet18-gated \\ \n",
    "# --repo_slim hawada/resnet18-slim \\ \n",
    "# --token hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \\ \n",
    "# --include_code adapters/torchvision,core,gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d27014-9b02-429a-80b9-6c0814ac008b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43940d3e-e7b1-4969-9566-5caf8f470341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90baaaab-355b-44c7-9651-c19b016a8f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hawada)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
