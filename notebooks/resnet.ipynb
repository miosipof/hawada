{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77bb846",
   "metadata": {},
   "source": [
    "# Example: ResNet-18 on RTX4090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105537a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "sys.path.append(str(pathlib.Path(\"resnet.ipynb\").resolve().parents[1]))\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f799eb7c",
   "metadata": {},
   "source": [
    "## Get a dataset for downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03059f52-87d3-42eb-be83-b9ecfce45a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kagglehub \n",
    "# import kagglehub\n",
    "# path = kagglehub.dataset_download(\"lyfora/processed-imagenet-dataset-224\")\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3476b93-d9e8-43c9-94ba-fa5b89ed7721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"/data/imagenet100-224/train\", exist_ok=True)\n",
    "# !mv /root/.cache/kagglehub/datasets/lyfora/processed-imagenet-dataset-224/versions/1 /data/imagenet100-224/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load / batch helpers\n",
    "from core.utils import load_yaml, _images_from_batch\n",
    "\n",
    "# Script to build config from a recipe\n",
    "from examples.run_resnet_optimize import build_from_recipe\n",
    "\n",
    "# Get needed metadata from recipe and download the base model\n",
    "pack = build_from_recipe(\"../recipes/RTX4090/resnet18_imagenet224.yaml\")\n",
    "\n",
    "print(\"Model name:\", pack['recipe']['model']['name'])\n",
    "print(\"Dataset root:\", pack['recipe']['data']['train_root'])\n",
    "print(\"Target GPU:\", pack[\"recipe\"]['target_gpu'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe0851",
   "metadata": {},
   "source": [
    "## Download snaphots: gated model + slim (pruned) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd54992-e0bb-451a-bb52-c93644e2d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "gated_model_repo  = \"hawada/resnet18-rtx4090-gated\"\n",
    "gated_local_dir   = \"ckpt/resnet/gated\"\n",
    "\n",
    "# Download pre-trained gated model (full weights + tuned gates for pruning)\n",
    "snapshot_download(repo_id=gated_model_repo, local_dir=gated_local_dir, repo_type=\"model\")\n",
    "\n",
    "\n",
    "slim_model_repo  = \"hawada/resnet18-rtx4090-slim\"\n",
    "slim_local_dir   = \"ckpt/resnet/slim\"\n",
    "\n",
    "# Download pre-trained slim model (already pruned)\n",
    "snapshot_download(repo_id=slim_model_repo, local_dir=slim_local_dir, repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b53c1d",
   "metadata": {},
   "source": [
    "### Use a simple custom loader to merge weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336c0af-fc16-4c58-93e7-fb10c00d478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.ckpt.resnet.slim.minimal_resnet_loader import load_student\n",
    "\n",
    "# Load slim model using a custom loader\n",
    "slim_model  = load_student(slim_local_dir+\"/pytorch_model.bin\", device=DEVICE)\n",
    "\n",
    "# Load gateg model using a custom loader\n",
    "gated_model = load_student(gated_local_dir+\"/pytorch_model.bin\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6d7579",
   "metadata": {},
   "source": [
    "## Measure latency: full ResNet vs Slim (pruned) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0179923c-7292-4409-950c-9294334bdb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters.torchvision.resnet import ResNetAdapter\n",
    "from core.profiler import measure_latency_ms\n",
    "\n",
    "B = pack[\"batch_size\"]; H = W = pack[\"img_size\"]\n",
    "\n",
    "print(f\"Starting benchmarking with batch size = {B}...\\n\")\n",
    "mean_keep, p95_keep, _ = measure_latency_ms(ResNetAdapter.export_keepall(gated_model), (B, 3, H, W), device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim_model, (B, 3, H, W), device=DEVICE)\n",
    "\n",
    "print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "if mean_keep > 0:\n",
    "    print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2dcb6",
   "metadata": {},
   "source": [
    "## Alternatively: Prune gated model with a custom export policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba03032-527b-48a6-bbb1-dc2bbcda315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters.torchvision.resnet import ResNetExportPolicy\n",
    "from core.export import Rounding as CoreRounding\n",
    "\n",
    "M = 2    # Multiples of this number will be used for pruned layers shapes\n",
    "K = 0.1  # Minimum ratio of kept shapes\n",
    "\n",
    "policy = ResNetExportPolicy(\n",
    "    warmup_steps=0,\n",
    "    rounding=CoreRounding(floor_groups=1, multiple_groups=M, min_keep_ratio=K),\n",
    "    min_keep_ratio=K,\n",
    ")\n",
    "\n",
    "# Obtain a new pruned model\n",
    "slim_model_new = ResNetAdapter.export_pruned(gated_model, policy, step=9999).to(DEVICE)\n",
    "\n",
    "print(f\"Starting benchmarking with batch size = {B}...\\n\")\n",
    "mean_keep, p95_keep, _ = measure_latency_ms(ResNetAdapter.export_keepall(gated_model), (B, 3, H, W), device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim_model_new, (B, 3, H, W), device=DEVICE)\n",
    "\n",
    "print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "if mean_keep > 0:\n",
    "    print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4609db",
   "metadata": {},
   "source": [
    "## Fine-tune a new slim model (distillation from teacher)\n",
    "\n",
    "After custom pruning and export, the model may lose its accuracy. Run fine-tuning script to restore the model's capacity on your downstream task (we use image classification on ImageNet in this notebook).\n",
    "\n",
    "[!] For this step you can use any other device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb94740a-dfc1-4b71-9288-2cde9f074e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.finetune import FinetuneConfig, finetune_student, recalibrate_bn_stats\n",
    "from core.distill import KDConfig\n",
    "\n",
    "# Get the suggested fine-tuning config from the recipe package\n",
    "\n",
    "teacher = pack[\"teacher\"] # Distallation from a bigger ResNet by default\n",
    "ft_epochs = int(pack[\"recipe\"].get(\"finetune\", {}).get(\"epochs\", 10))\n",
    "learning_rate = float(pack[\"recipe\"].get(\"finetune\", {}).get(\"lr\", 3e-4))\n",
    "weight_decay = float(pack[\"recipe\"].get(\"finetune\", {}).get(\"wd\", 1e-5))\n",
    "\n",
    "train_loader = pack[\"train_loader\"]\n",
    "val_loader   = pack[\"val_loader\"]\n",
    "\n",
    "# Build a config for fine-tuning\n",
    "ft_cfg = FinetuneConfig(\n",
    "    epochs=ft_epochs,\n",
    "    lr=learning_rate,\n",
    "    wd=weight_decay,\n",
    "    kd=KDConfig(**pack[\"recipe\"].get(\"trainer\", {}).get(\"kd\", {})),\n",
    "    amp=bool(pack[\"recipe\"].get(\"trainer\", {}).get(\"amp\", True)),\n",
    "    mse_weight = float(pack[\"recipe\"].get(\"trainer\", {}).get(\"mse_weight\", 0.0)),\n",
    "    device=DEVICE,\n",
    "    log_every=50,\n",
    ")\n",
    "\n",
    "print(f\"\\nStarting fine tuning for {ft_epochs} epochs, LR={learning_rate} ...\\n\")\n",
    "slim_finetuned = finetune_student(\n",
    "    slim_model_new,\n",
    "    teacher,\n",
    "    train_loader,\n",
    "    get_student_logits=lambda m, batch: m(_images_from_batch(batch)),\n",
    "    get_teacher_logits=lambda m, batch: m(_images_from_batch(batch)).detach(),\n",
    "    cfg=ft_cfg,\n",
    "    val_loader=val_loader,\n",
    ")\n",
    "\n",
    "# Recalibrate BatchNorm stats before export\n",
    "recalibrate_bn_stats(slim_finetuned, train_loader, max_batches=1000)\n",
    "\n",
    "# # Now you have a faster model that behaves well on the selected dataset\n",
    "# import torch\n",
    "# out_path = \"resnet18_slim_new.pth\"\n",
    "# torch.save(slim_finetuned.state_dict(), out_path)\n",
    "# print(\"Saved pruned model to\", out_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f5d70",
   "metadata": {},
   "source": [
    "## Train a new gated model on your GPU\n",
    "\n",
    "HawAda framework allows you to optimize the model for your particular GPU. To do that, you have to train gates on your device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31827c-1a5c-456f-a1c4-a316d52d439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optionally clean the memory\n",
    "\n",
    "# import gc\n",
    "\n",
    "# del gated_model, slim_model, slim_model_new, slim_finetuned\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bb1109-e949-4d64-a70c-d549b4a1fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.train import TrainerConfig\n",
    "\n",
    "\n",
    "os.makedirs(\"runs/resnet18\", exist_ok=True)\n",
    "\n",
    "# Student to prune and teracher to distill\n",
    "student = pack[\"student\"] # A new instance of ResNet-18\n",
    "teacher = pack[\"teacher\"] # A bigger ResNet-50\n",
    "\n",
    "# Loaders built via ImageNet224 dataset\n",
    "train_loader = pack[\"train_loader\"]\n",
    "val_loader   = pack[\"val_loader\"]\n",
    "\n",
    "# Differentiable proxy to estimate latency\n",
    "proxy = pack[\"proxy\"]\n",
    "\n",
    "# Pruning policy\n",
    "export_policy = pack[\"export_policy\"]\n",
    "\n",
    "# Training configuration\n",
    "train_cfg = pack[\"trainer_cfg\"]\n",
    "\n",
    "\n",
    "print(\"Pruning policy: multiple =\", export_policy.rounding.multiple_groups)\n",
    "print(\"Pruning policy: min keep ratio =\", export_policy.rounding.min_keep_ratio)\n",
    "\n",
    "print(\"\\nTraining conf: early stopping patience =\", train_cfg.early_stopping_patience)\n",
    "print(\"Training conf: LR for gates =\", train_cfg.lr_gate)\n",
    "print(\"Training conf: LR for linear layers =\", train_cfg.lr_linear)\n",
    "print(\"Training conf: LR for affine layers =\", train_cfg.lr_affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6f1b65-9d61-46e3-b7b2-7e7c7a23c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.train import LagrangeTrainer\n",
    "\n",
    "# Build training configuration from the recipe package\n",
    "trainer = LagrangeTrainer(\n",
    "    student=student,\n",
    "    teacher=teacher,\n",
    "    proxy=proxy,  \n",
    "    adapter_get_student_logits=lambda m, batch: m(_images_from_batch(batch)),\n",
    "    adapter_get_teacher_logits=lambda m, batch: m(_images_from_batch(batch)).detach(),\n",
    "    adapter_export_keepall=ResNetAdapter.export_keepall,\n",
    "    adapter_export_pruned=lambda m, pol, step: ResNetAdapter.export_pruned(m, pol, step),\n",
    "    export_policy=export_policy,\n",
    "    cfg=train_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f330d9-2cfb-456d-9a5e-d2ef46f59ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "\n",
    "lambdas = []\n",
    "for ep in range(EPOCHS):\n",
    "    print(f\"=== Epoch {ep+1}/{EPOCHS} ===\")\n",
    "    lam = trainer.train_epoch(train_loader)\n",
    "    lambdas.append(lam)\n",
    "\n",
    "    last = lambdas[:train_cfg.early_stopping_patience]\n",
    "    last = [x for x in last if x < train_cfg.early_stopping_lambda]\n",
    "    if len(last) == train_cfg.early_stopping_patience:\n",
    "        print(f\"Early stopping: lambda < {train_cfg.early_stopping_lambda} for last {train_cfg.early_stopping_patience} epochs\")\n",
    "        break\n",
    "\n",
    "\n",
    "# # Save gated model        \n",
    "# out_path = os.path.join(\"runs/resnet\", \"resnet18_gated.pth\")\n",
    "# torch.save(student.state_dict(), out_path)\n",
    "# print(\"Saved gated model to\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6bb5f",
   "metadata": {},
   "source": [
    "## Grid search the best pruning multiples on your GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24837c6d-eaed-49e5-9f11-39191ccaeba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from examples.run_resnet_optimize import grid_search_export\n",
    "\n",
    "print(\"Running export grid search...\")\n",
    "\n",
    "# Multiples - set up accorging to your GPU or use recipe\n",
    "multiples = pack[\"recipe\"].get(\"export\").get(\"grid_multiple_groups\")\n",
    "\n",
    "res = grid_search_export(\n",
    "    student, # Gated model with pre-trained gate weights\n",
    "    device=DEVICE,\n",
    "    img_size=pack[\"img_size\"],\n",
    "    B=pack[\"batch_size\"],\n",
    "    multiples=multiples,\n",
    "    min_keep_ratio=float(pack[\"recipe\"].get(\"trainer\").get(\"lagrange\").get(\"min_keep_ratio\")),\n",
    ")\n",
    "\n",
    "\n",
    "# Select the fastest model\n",
    "slim = res[\"best_model\"]\n",
    "print(\"Best export params:\", res[\"best_params\"])\n",
    "\n",
    "# BatchNorm recalibration before saving the model\n",
    "print(\"\\nRecalibrating BN stats on the slim model...\")\n",
    "ResNetAdapter.bn_recalibration(slim, train_loader, num_batches=1000, device=DEVICE)\n",
    "print(\"Done\")\n",
    "\n",
    "# # Save model\n",
    "# out_path = os.path.join(\"runs/resnet\", \"resnet18_slim.pth\")\n",
    "# torch.save(slim, out_path)\n",
    "# print(\"Saved pruned model to\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f52ad2",
   "metadata": {},
   "source": [
    "## Measure latency of your new slim model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cc9f00-7fb8-434f-b440-296b451f5041",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = pack[\"batch_size\"]; H = W = pack[\"img_size\"]\n",
    "\n",
    "print(f\"Starting benchmarking with batch size = {B}...\\n\")\n",
    "mean_keep, p95_keep, _ = measure_latency_ms(ResNetAdapter.export_keepall(student), (B, 3, H, W), device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim, (B, 3, H, W), device=DEVICE)\n",
    "\n",
    "print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "if mean_keep > 0:\n",
    "    print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8f1d5",
   "metadata": {},
   "source": [
    "Now you have a slim model optimized for your GPU. \n",
    "You can fine-tune it on your downstream task with a teacher, as we did above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f44b6",
   "metadata": {},
   "source": [
    "## Export to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d5fcbc-09eb-495b-81cc-4731740f8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m tools.export_to_hf \\ \n",
    "# --task resnet \\ \n",
    "# --base_id torchvision/resnet18 \\ \n",
    "# --student_ckpt runs/resnet18/resnet18_gated.pth \\ \n",
    "# --slim_ckpt runs/resnet18/resnet18_slim.pth \\ \n",
    "# --repo_gated hawada/resnet18-gated \\ \n",
    "# --repo_slim hawada/resnet18-slim \\ \n",
    "# --token hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \\ \n",
    "# --include_code adapters/torchvision,core,gates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
