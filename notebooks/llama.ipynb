{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ae1002",
   "metadata": {},
   "source": [
    "# Example: LlaMa-3.2-1B on RTX4090 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce87912-2116-43fc-b3a0-ff0390040696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "sys.path.append(str(pathlib.Path(\"resnet.ipynb\").resolve().parents[1]))\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccc80c-de8c-49f3-a5a5-2a720e4bdc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b9150",
   "metadata": {},
   "source": [
    "## Build model with trainable gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c02cff-e7ae-4604-94d0-ca49fa8817ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.profiler import measure_latency_text_ms \n",
    "\n",
    "from adapters.huggingface.llama import (\n",
    "    LlamaAdapter,\n",
    "    LlamaGatingConfig,\n",
    "    LlamaExportPolicy,\n",
    "    LatencyProxyLLM,\n",
    "    infer_slim_meta,\n",
    "    load_slim_for_finetune\n",
    ")\n",
    "\n",
    "# Script to build config from a recipe\n",
    "from examples.run_llama_optimize import build_from_recipe, _ids_mask, _ProxyBridge\n",
    "\n",
    "# Get needed metadata from recipe and download the base model\n",
    "pack = build_from_recipe(\"../recipes/RTX4090/llama_3_2_1b.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1a563-9a2d-4bd6-8af0-5274ba7e8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_model = pack[\"student\"] # LlaMa-3.2-1B\n",
    "teacher     = pack[\"teacher\"] # Another instance of LlaMa-3.2-1B\n",
    "\n",
    "# HawAda adapter to train gates\n",
    "adapter = LlamaAdapter(student)\n",
    "\n",
    "# Gates config\n",
    "gate_cfg = LlamaGatingConfig(\n",
    "    tau=float(pack.get(\"gating\").get(\"tau\")), # Target latency (respect to base latency)\n",
    "    init_logit=float(pack.get(\"gating\").get(\"init_logit\")),  # Initial logit for gate sigmoids\n",
    "    head_gating=bool(pack.get(\"gating\").get(\"head_gating\")), # If to gate heads or not\n",
    "    gate_kv=bool(pack.get(\"gating\").get(\"gate_kv\")),  # If to gate KV layers or not (default=False)\n",
    "    ffn_group=int(pack.get(\"gating\").get(\"ffn_group\")), # Feed-forward network group size\n",
    "    ffn_gating=bool(pack.get(\"gating\", {}).get(\"ffn_gating\")), # If to gate FFN layers or not\n",
    "    hard_eval=bool(pack.get(\"gating\", {}).get(\"hard_eval\")),\n",
    ")\n",
    "\n",
    "# Attach gates to student\n",
    "gated_model = adapter.attach_gates(gate_cfg).train().to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8878e76",
   "metadata": {},
   "source": [
    "### Download pre-trained gates from HawAda repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909cece-233d-4f55-825a-49f1a5f910d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path  = hf_hub_download(gated_model_repo, \"pytorch_model.bin\")\n",
    "state_dict = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "missing, unexpected = gated_model.load_state_dict(state_dict, strict=False)\n",
    "print(\"missing:\", len(missing), \"unexpected:\", len(unexpected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810745cf",
   "metadata": {},
   "source": [
    "### Prune and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f7203-f1c5-4041-8bb4-34a2f97c5125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check configuration for pruning and export\n",
    "print(\"Policy for the probes during training:\", pack[\"probe_policy\"])\n",
    "print(\"\\nPolicy for the final pruning:\", pack[\"export_policy\"])\n",
    "\n",
    "slim_model = adapter.export_pruned(gated_model, \n",
    "                                   policy=pack[\"export_policy\"], \n",
    "                                   step=9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fa5a1c",
   "metadata": {},
   "source": [
    "## Measure latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103eb30-6837-4113-b768-fe23df9f0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher      = pack[\"teacher\"] # Another instance of LlaMa-3.2-1B\n",
    "train_loader = pack[\"train_loader\"]\n",
    "val_loader   = pack[\"val_loader\"]\n",
    "llm_proxy    = pack[\"proxy\"]\n",
    "decode_T     = pack[\"decode_T\"]\n",
    "\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "B, S = int(ids.size(0)), int(ids.size(1))\n",
    "lat_cfg = cfg.get(\"latency\", {})\n",
    "decode_T = int(lat_cfg.get(\"decode_T_tokens\", 128))\n",
    "\n",
    "print(f\"Starting benchmarking with batch size = {B}, S = {S}, decode_T = {decode_T}...\")\n",
    "mean_keep, p95_keep, _ = measure_latency_text_ms(teacher.to(device).eval(), B=B, S=S, T=decode_T, device=device)\n",
    "mean_slim, p95_slim, _ = measure_latency_text_ms(slim.to(device).eval(), B=B, S=S, T=decode_T, device=device)\n",
    "print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "if mean_keep > 0:\n",
    "    print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d95727",
   "metadata": {},
   "source": [
    "Now you your own LlaMa version, optimized for RTX4090.\n",
    "\n",
    "You will want to fine-tune it for your dowstream task; this can be done on any other device (e.g., H100).\n",
    "\n",
    "HawAda framework allows you to optimize the model for other GPUs; To do it, follow the following steps:\n",
    "\n",
    "* Create your own recipe\n",
    "* Attach HawAda adapter to the model\n",
    "* Run the gates training on your device (see ResNet notebook)\n",
    "* Export pruned model after gates are trained\n",
    "* Run grid search to choose the best shapes\n",
    "* Run distillation (fine tuning) for your downstream task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
