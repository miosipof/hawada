{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c25fb5f7-2530-4eb8-943f-5ad338da5838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: optimize LlaMa-3.2-1B for RTX4090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce87912-2116-43fc-b3a0-ff0390040696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "sys.path.append(str(pathlib.Path(\"resnet.ipynb\").resolve().parents[1]))\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6eccc80c-de8c-49f3-a5a5-2a720e4bdc2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d0689a3b8d416e899d94afd357428c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gc\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b2f97-aba9-4ef7-a39e-2c34c6076dce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d84859c-23c8-48b2-888a-bf72200ad6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model_repo  = \"hawada/Llama-3.2-1B-rtx4090-slim\"\n",
    "gated_model_repo = \"hawada/Llama-3.2-1B-rtx4090-gated\"\n",
    "\n",
    "# slim_model_repo  = \"hawada/Llama-3.2-1B-h100-slim\"\n",
    "# gated_model_repo = \"hawada/Llama-3.2-1B-h100-gated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c02cff-e7ae-4604-94d0-ca49fa8817ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.profiler import measure_latency_text_ms \n",
    "\n",
    "from adapters.huggingface.llama import (\n",
    "    LlamaAdapter,\n",
    "    LlamaGatingConfig,\n",
    "    LlamaExportPolicy,\n",
    "    LatencyProxyLLM,\n",
    "    infer_slim_meta,\n",
    "    load_slim_for_finetune\n",
    ")\n",
    "\n",
    "# Script to build config from a recipe\n",
    "from examples.run_llama_optimize import build_from_recipe, _ids_mask, _ProxyBridge\n",
    "\n",
    "# Get needed metadata from recipe and download the base model\n",
    "pack = build_from_recipe(\"../recipes/RTX4090/llama_3_2_1b.yaml\")\n",
    "\n",
    "# print(\"Student:\", pack[\"recipe\"][\"model\"][\"name\"])\n",
    "# print(\"Teacher:\", pack[\"recipe\"][\"base_model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0e1a563-9a2d-4bd6-8af0-5274ba7e8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_model = pack[\"student\"] # LlaMa-3.2-1B\n",
    "teacher     = pack[\"teacher\"] # Another instance of LlaMa-3.2-1B\n",
    "\n",
    "# HawAda adapter to train gates\n",
    "adapter = LlamaAdapter(student)\n",
    "\n",
    "# Gates config\n",
    "gate_cfg = LlamaGatingConfig(\n",
    "    tau=float(pack.get(\"gating\").get(\"tau\")), # Target latency (respect to base latency)\n",
    "    init_logit=float(pack.get(\"gating\").get(\"init_logit\")),  # Initial logit for gate sigmoids\n",
    "    head_gating=bool(pack.get(\"gating\").get(\"head_gating\")), # If to gate heads or not\n",
    "    gate_kv=bool(pack.get(\"gating\").get(\"gate_kv\")),  # If to gate KV layers or not (default=False)\n",
    "    ffn_group=int(pack.get(\"gating\").get(\"ffn_group\")), # Feed-forward network group size\n",
    "    ffn_gating=bool(pack.get(\"gating\", {}).get(\"ffn_gating\")), # If to gate FFN layers or not\n",
    "    hard_eval=bool(pack.get(\"gating\", {}).get(\"hard_eval\")),\n",
    ")\n",
    "\n",
    "# Attach gates to student\n",
    "gated_model = adapter.attach_gates(gate_cfg).train().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7909cece-233d-4f55-825a-49f1a5f910d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing: 0 unexpected: 0\n"
     ]
    }
   ],
   "source": [
    "ckpt_path  = hf_hub_download(gated_model_repo, \"pytorch_model.bin\")\n",
    "state_dict = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "missing, unexpected = gated_model.load_state_dict(state_dict, strict=False)\n",
    "print(\"missing:\", len(missing), \"unexpected:\", len(unexpected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c1f7203-f1c5-4041-8bb4-34a2f97c5125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for the probes during training: LlamaExportPolicy(warmup_steps=0, head_rounding=Rounding(floor_groups=1, multiple_groups=1, min_keep_ratio=0.5), ffn_rounding=Rounding(floor_groups=1, multiple_groups=1, min_keep_ratio=0.5), q_rounding=Rounding(floor_groups=1, multiple_groups=1, min_keep_ratio=0.5))\n",
      "\n",
      "Policy for the final pruning: LlamaExportPolicy(warmup_steps=200, head_rounding=Rounding(floor_groups=16, multiple_groups=4, min_keep_ratio=0.5), ffn_rounding=Rounding(floor_groups=1, multiple_groups=16, min_keep_ratio=0.5), q_rounding=Rounding(floor_groups=4, multiple_groups=8, min_keep_ratio=0.5))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LlamaAttention' object has no attribute 'num_heads'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPolicy for the probes during training:\u001b[39m\u001b[33m\"\u001b[39m, pack[\u001b[33m\"\u001b[39m\u001b[33mprobe_policy\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPolicy for the final pruning:\u001b[39m\u001b[33m\"\u001b[39m, pack[\u001b[33m\"\u001b[39m\u001b[33mexport_policy\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m slim_model = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexport_pruned\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgated_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpack\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexport_policy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                                   \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m9999\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/hawada/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/hawada/adapters/huggingface/llama.py:656\u001b[39m, in \u001b[36mLlamaAdapter.export_pruned\u001b[39m\u001b[34m(model_with_gates, policy, step)\u001b[39m\n\u001b[32m    654\u001b[39m     new_attn.head_dim = \u001b[38;5;28mint\u001b[39m(Dh)\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(new_attn, \u001b[33m\"\u001b[39m\u001b[33mnum_key_value_groups\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m     new_attn.num_key_value_groups = \u001b[43mnew_attn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_heads\u001b[49m // new_attn.num_key_value_heads\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# plug back\u001b[39;00m\n\u001b[32m    659\u001b[39m layer.self_attn = new_attn\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/hawada/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1964\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1963\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1965\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'LlamaAttention' object has no attribute 'num_heads'"
     ]
    }
   ],
   "source": [
    "# Check configuration for pruning and export\n",
    "print(\"Policy for the probes during training:\", pack[\"probe_policy\"])\n",
    "print(\"\\nPolicy for the final pruning:\", pack[\"export_policy\"])\n",
    "\n",
    "slim_model = adapter.export_pruned(gated_model, \n",
    "                                   policy=pack[\"export_policy\"], \n",
    "                                   step=9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d103eb30-6837-4113-b768-fe23df9f0a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher      = pack[\"teacher\"] # Another instance of LlaMa-3.2-1B\n",
    "train_loader = pack[\"train_loader\"]\n",
    "val_loader   = pack[\"val_loader\"]\n",
    "llm_proxy    = pack[\"proxy\"]\n",
    "decode_T     = pack[\"decode_T\"]\n",
    "\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "B, S = int(ids.size(0)), int(ids.size(1))\n",
    "lat_cfg = cfg.get(\"latency\", {})\n",
    "decode_T = int(lat_cfg.get(\"decode_T_tokens\", 128))\n",
    "\n",
    "print(f\"Starting benchmarking with batch size = {B}, S = {S}, decode_T = {decode_T}...\")\n",
    "mean_keep, p95_keep, _ = measure_latency_text_ms(teacher.to(device).eval(), B=B, S=S, T=decode_T, device=device)\n",
    "mean_slim, p95_slim, _ = measure_latency_text_ms(slim.to(device).eval(), B=B, S=S, T=decode_T, device=device)\n",
    "print(f\"Base: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms\")\n",
    "print(f\"Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms\\n\")\n",
    "if mean_keep > 0:\n",
    "    print(f\"Speedup={100.0*(mean_keep-mean_slim)/mean_keep:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec8e7e-9e58-421e-9975-d8f7248aae70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd78b0e-34c0-4794-9815-efdbb1cf92b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a157285-084d-4608-89b0-d7de10b574ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a65107-9519-4368-baaf-59794e6b9ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b1985-1c86-4ebe-b041-3a5b38760cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085827e-8a85-4a5c-8cb4-37e05422c60b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50a38f-f633-4988-9d88-c3f58332938e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e667ec-fac5-4356-bd0d-efe4d4e50625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17bbe1d-69cc-45d0-8b86-b01a3f9f0f28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f2b1b-f6b2-464d-a280-e8ff6088d596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39de7546-3abe-4af4-9801-484089da4337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713b611-0970-4e3a-a073-c074bb77cd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f672ff9-c1ea-4cb1-a841-40ef6b8abd6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc55d0f-6a8b-4638-bb06-52602630c0d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73357753-6528-40e9-ab4d-9e748421b50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6b487-731d-41d8-82e8-1690427e6b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345c2ef-229b-4601-9044-ee9c843baa1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aecc3c-4128-42f1-afc2-be12db5085e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2198592-80c7-469b-9208-4452fb21d3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69192934-6103-4598-9aa2-27bf38af9cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5d64d03-6827-4676-a473-7edbc7f3c317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7218be-e4b6-4e2d-bb30-4485ca579a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calibrate the proxy function for latency measurement on current GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bde232c-02b3-4ffd-9a19-827135f7d1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[calib] keep-all measured ≈ 1654.990 ms; proxy.scale_ms set to 2.195300e-09\n"
     ]
    }
   ],
   "source": [
    "train_loader = pack[\"train_loader\"]\n",
    "llm_proxy    = pack[\"proxy\"]\n",
    "decode_T     = pack[\"decode_T\"]\n",
    "\n",
    "# Proxy calibration\n",
    "batch = next(iter(train_loader))\n",
    "B, S = int(ids.size(0)), int(ids.size(1))\n",
    "\n",
    "# measure real keep-all\n",
    "slim_keepall = adapter.export_keepall(student).to(DEVICE).eval()\n",
    "real_ms, _, _ = measure_latency_text_ms(slim_keepall, B=B, S=S, T=decode_T, device=DEVICE)\n",
    "del slim_keepall; gc.collect()\n",
    "\n",
    "# proxy's raw keep-all prediction (pre-scale)\n",
    "raw_pred = llm_proxy.predict(student, batch)\n",
    "raw_pred = float(raw_pred.detach().item() if hasattr(raw_pred, \"detach\") else raw_pred)\n",
    "raw_pred = max(raw_pred, 1e-9)\n",
    "\n",
    "llm_proxy.scale_ms = max(1e-9, float(real_ms) / raw_pred)\n",
    "print(f\"[calibration] keep-all measured ≈ {real_ms:.3f} ms; proxy.scale_ms set to {llm_proxy.scale_ms:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67d681-3c9c-4c74-95b2-ba56f889a5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hawada)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
