{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e6f7ec-0a96-473d-93b9-2d7572286aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: optimize ViT-base-16 for RTX4090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b91d598-5563-4886-b053-7a09e175d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "sys.path.append(str(pathlib.Path(\"resnet.ipynb\").resolve().parents[1]))\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a46a682-8f4c-499a-8afc-4a2be8ce391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from huggingface_hub import notebook_login, hf_hub_download\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9eb104f3-dc7c-4d37-9b31-349688baac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model_repo  = \"hawada/vit-base-patch16-224-rtx4090-slim\"\n",
    "gated_model_repo = \"hawada/vit-base-patch16-224-rtx4090-gated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bca9b56-00f2-4db3-9710-5ca0071d8a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency proxy scale set to: 9.856479e-09\n",
      "Keep-all latency: 169.4334 ms on batch size = 64\n",
      "Target latency = 169.4334 * 0.60 ~ 101.6600\n"
     ]
    }
   ],
   "source": [
    "from core.profiler import measure_latency_ms\n",
    "from data.vision import build_imagenet_like_loaders, VisionDataConfig, _images_from_batch\n",
    "\n",
    "from adapters.huggingface.vit import (\n",
    "    ViTAdapter,\n",
    "    ViTGatingConfig,\n",
    "    ViTExportPolicy,\n",
    "    ViTLatencyProxy,\n",
    "    ViTProxyConfig,\n",
    "    ViTGrid,\n",
    "    vit_search_best_export,\n",
    "    SlimViTForImageClassification\n",
    ")\n",
    "\n",
    "# Script to build config from a recipe\n",
    "from examples.run_vit_optimize import build_from_recipe, make_vit_policy\n",
    "\n",
    "# Get needed metadata from recipe and download the base model\n",
    "pack = build_from_recipe(\"../recipes/RTX4090/vit_base_patch16_224.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "793e39a7-6754-4642-898a-17eb0cffd103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['student', 'teacher', 'student_head', 'teacher_head', 'adapter', 'export_policy', 'probe_policy', 'proxy', 'trainer_cfg', 'train_loader', 'val_loader', 'get_s', 'get_t', 'img_size', 'batch_size', 'device', 'recipe'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pack.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70475cf3-2771-4035-8898-05151c5a23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_model = pack[\"student\"].to(DEVICE) # ViT with attached gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2d401d4-d2ba-4e79-be39-01f2f8a8cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = pack[\"img_size\"]   # Image size\n",
    "B = pack[\"batch_size\"]        # Recommended batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6ec631-4533-4d45-8e62-da458fca3c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50346103-1897-4d37-98a8-ff548294f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a pruned model from Huggingface (optimised for RTX4090)\n",
    "slim_model = SlimViTForImageClassification.from_pretrained(slim_model_repo).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503a59ff-1fba-4ba8-9c06-aca83db97619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8de53d22-c41f-4f91-9202-6416beacfb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting benchmarking with batch size = 64...\n",
      "Keep-all: mean=170.051ms p95=184.496ms | Slim: mean=148.580ms p95=162.181ms | \n",
      "Speedup=12.63%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting benchmarking with batch size = {B}...\")\n",
    "\n",
    "full_model = ViTAdapter.export_keepall(gated_model).to(DEVICE)\n",
    "shape = (B, 3, img_size, img_size)\n",
    "    \n",
    "mean_keep, p95_keep, _ = measure_latency_ms(full_model, shape, device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim_model, shape, device=DEVICE)\n",
    "\n",
    "print(f\"Keep-all: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms | Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms | \\n\"\n",
    "      f\"Speedup={(mean_keep-mean_slim)/max(1e-6,mean_keep)*100:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc93090-8973-45d2-84ca-353e19eb7c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840537b3-38e7-4f4f-8cc9-4963faaa0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your own slim model with custom export parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1c7e50-6163-40fb-86e6-df700bc9719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing: 0 unexpected: 0\n"
     ]
    }
   ],
   "source": [
    "# Download pre-trained gates for ViT on RTX4090\n",
    "\n",
    "ckpt_path  = hf_hub_download(gated_model_repo, \"pytorch_model.bin\")\n",
    "state_dict = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "missing, unexpected = gated_model.load_state_dict(state_dict, strict=False)\n",
    "print(\"missing:\", len(missing), \"unexpected:\", len(unexpected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58cc242a-c47a-4369-8766-dd3576f911d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy for the probes during training: ExportPolicy(warmup_steps=0, rounding=Rounding(floor_groups=1, multiple_groups=1, min_keep_ratio=0.0))\n",
      "\n",
      "Policy for the final pruning: ExportPolicy(warmup_steps=150, rounding=Rounding(floor_groups=1, multiple_groups=1, min_keep_ratio=0.0))\n"
     ]
    }
   ],
   "source": [
    "# Check configuration for pruning and export\n",
    "print(\"Policy for the probes during training:\", pack[\"probe_policy\"])\n",
    "print(\"\\nPolicy for the final pruning:\", pack[\"export_policy\"])\n",
    "\n",
    "slim_model = ViTAdapter.export_pruned(gated_model, \n",
    "                                   policy=pack[\"export_policy\"], \n",
    "                                   step=9999).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5af590e6-8828-4f47-b96d-32a4d076e603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/11] head_multiple 2 | ffn_snap 1 | mean_ms = 143.0825799560547\n",
      "Best export params: {'head_multiple': 2, 'ffn_snap': 1}\n"
     ]
    }
   ],
   "source": [
    "# Run the grid search for the best export parameters\n",
    "\n",
    "num_heads = int(gated_model.config.num_attention_heads)\n",
    "grid_cfg = pack[\"recipe\"].get(\"export\").get(\"search\")\n",
    "head_grid = tuple(grid_cfg.get(\"grid_multiple_groups\"))\n",
    "ffn_grid  = tuple(grid_cfg.get(\"ffn_snaps\"))       \n",
    "\n",
    "search = vit_search_best_export(\n",
    "    gated_model.to(DEVICE),\n",
    "    export_fn=ViTAdapter.export_pruned,\n",
    "    num_heads=num_heads,\n",
    "    step=9999,  # no warmup\n",
    "    batch_shape=(B, 3, img_size, img_size),\n",
    "    device=pack[\"device\"],\n",
    "    make_policy=make_vit_policy,\n",
    "    grid=ViTGrid(head_multiple_grid=head_grid, ffn_snap_grid=ffn_grid),\n",
    ")\n",
    "\n",
    "\n",
    "slim_model = search.best_model.to(DEVICE)\n",
    "print(\"Best export params:\", search.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8880c938-aff8-4abd-96a4-715702676d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting benchmarking with batch size = 64...\n",
      "Keep-all: mean=172.554ms p95=185.943ms | Slim: mean=145.194ms p95=156.236ms | \n",
      "Speedup=15.86%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting benchmarking with batch size = {B}...\")\n",
    "    \n",
    "full_model = ViTAdapter.export_keepall(gated_model).to(DEVICE)\n",
    "shape = (B, 3, img_size, img_size)\n",
    "    \n",
    "mean_keep, p95_keep, _ = measure_latency_ms(full_model, shape, device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim_model, shape, device=DEVICE)\n",
    "\n",
    "print(f\"Keep-all: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms | Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms | \\n\"\n",
    "      f\"Speedup={(mean_keep-mean_slim)/max(1e-6,mean_keep)*100:.2f}%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d7cca-69fd-4ff2-a41e-c0a4e89a0e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1233e39b-c15a-4029-a780-f90df4a9b05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This slim model needs fine-tuning for downstream task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6e6311f-8974-4237-ac32-53c90bd4008b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting fine tuning for 1 epochs...\n",
      "[AMP] Mode=BF16 | GradScaler=OFF | KD: T=4.0 alpha=2.0 | LR=0.0001 WD=0.0 | Trainable params=69,938,404\n",
      "Step 200/1625 (ep 1/1): running loss = 4.3769\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting fine tuning for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mft_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m ft_cfg = FinetuneConfig(\n\u001b[32m     10\u001b[39m     epochs=ft_epochs,\n\u001b[32m     11\u001b[39m     lr=\u001b[38;5;28mfloat\u001b[39m(pack[\u001b[33m\"\u001b[39m\u001b[33mrecipe\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mfinetune\u001b[39m\u001b[33m\"\u001b[39m).get(\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m)),\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     log_every=\u001b[32m200\u001b[39m,\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m slim = \u001b[43mfinetune_student\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mslim_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mteacher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpack\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain_loader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_student_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpack\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget_s\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_teacher_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpack\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget_t\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mft_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpack\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mval_loader\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_best\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# torch.save(slim, os.path.join(args.outdir, \"vit_slim_finetune.pth\"))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/hawada/core/finetune.py:197\u001b[39m, in \u001b[36mfinetune_student\u001b[39m\u001b[34m(student, teacher, train_loader, get_student_logits, get_teacher_logits, cfg, task_loss, val_loader, on_step, save_best)\u001b[39m\n\u001b[32m    195\u001b[39m     scaler.update()\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     opt.step()\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# ---- diagnostics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/hawada/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/hawada/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/hawada/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from core.finetune import FinetuneConfig, finetune_student\n",
    "from core.distill import KDConfig\n",
    "\n",
    "teacher = pack[\"teacher\"].to(DEVICE) # Another instance of ViT for optional training / fine tuning\n",
    "\n",
    "ft_epochs = 1 # int(pack[\"recipe\"].get(\"finetune\").get(\"epochs\")\n",
    "print(f\"\\nStarting fine tuning for {ft_epochs} epochs...\")\n",
    "\n",
    "ft_cfg = FinetuneConfig(\n",
    "    epochs=ft_epochs,\n",
    "    lr=float(pack[\"recipe\"].get(\"finetune\").get(\"lr\")),\n",
    "    kd=KDConfig(**pack[\"recipe\"].get(\"trainer\").get(\"kd\")),\n",
    "    amp=bool(pack[\"recipe\"].get(\"trainer\").get(\"amp\")),\n",
    "    device=pack[\"device\"],\n",
    "    log_every=200,\n",
    ")\n",
    "\n",
    "slim = finetune_student(\n",
    "    slim_model,\n",
    "    teacher,\n",
    "    pack[\"train_loader\"],\n",
    "    get_student_logits=pack[\"get_s\"],\n",
    "    get_teacher_logits=pack[\"get_t\"],\n",
    "    cfg=ft_cfg,\n",
    "    val_loader=pack[\"val_loader\"],\n",
    "    save_best=True\n",
    ")\n",
    "\n",
    "\n",
    "# torch.save(slim, os.path.join(args.outdir, \"vit_slim_finetune.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f01f20-28af-4c1c-a832-f0121c055c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eae59e-98b4-4fab-8c58-3cdc15d72864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549a3b1-0d89-4443-bcb4-4210aca589bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6a191-c1ea-468b-a13f-4e1d87f43d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f09f0-683a-4da1-9631-a81fffa20ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ac3ea-9fbb-4ad8-827d-216990d6f13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd534cfd-53df-46eb-a6ff-b48e733be596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hawada)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
