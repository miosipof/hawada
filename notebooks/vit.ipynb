{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efb801d7",
   "metadata": {},
   "source": [
    "# Example: ViT-base-16 on RTX4090 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91d598-5563-4886-b053-7a09e175d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "sys.path.append(str(pathlib.Path(\"resnet.ipynb\").resolve().parents[1]))\n",
    "\n",
    "DEVICE = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a46a682-8f4c-499a-8afc-4a2be8ce391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from huggingface_hub import notebook_login, hf_hub_download\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee658df",
   "metadata": {},
   "source": [
    "## Build model with trainable gates\n",
    "\n",
    "First we will use it without any pruning to compare latency with a ready-to-use optimized model from HawAda repo.\n",
    "\n",
    "Then we will use gates to export our own \"slim\" model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca9b56-00f2-4db3-9710-5ca0071d8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.profiler import measure_latency_ms\n",
    "from data.vision import build_imagenet_like_loaders, VisionDataConfig, _images_from_batch\n",
    "\n",
    "from adapters.huggingface.vit import (\n",
    "    ViTAdapter,\n",
    "    ViTGatingConfig,\n",
    "    ViTExportPolicy,\n",
    "    ViTLatencyProxy,\n",
    "    ViTProxyConfig,\n",
    "    ViTGrid,\n",
    "    vit_search_best_export,\n",
    "    SlimViTForImageClassification\n",
    ")\n",
    "\n",
    "# Script to build config from a recipe\n",
    "from examples.run_vit_optimize import build_from_recipe, make_vit_policy\n",
    "\n",
    "# Get needed metadata from recipe and download the base model\n",
    "pack = build_from_recipe(\"../recipes/RTX4090/vit_base_patch16_224.yaml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70475cf3-2771-4035-8898-05151c5a23b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_model = pack[\"student\"].to(DEVICE) # ViT with attached gates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d401d4-d2ba-4e79-be39-01f2f8a8cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = pack[\"img_size\"]   # Image size\n",
    "B = pack[\"batch_size\"]        # Recommended batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8c2f76",
   "metadata": {},
   "source": [
    "## Get a pruned version from HawAda collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50346103-1897-4d37-98a8-ff548294f5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "slim_model_repo  = \"hawada/vit-base-patch16-224-rtx4090-slim\"\n",
    "\n",
    "# Get a pruned model from Huggingface (optimised for RTX4090)\n",
    "slim_model = SlimViTForImageClassification.from_pretrained(slim_model_repo).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756a11c",
   "metadata": {},
   "source": [
    "### Measure latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de53d22-c41f-4f91-9202-6416beacfb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting benchmarking with batch size = {B}...\")\n",
    "\n",
    "full_model = ViTAdapter.export_keepall(gated_model).to(DEVICE)\n",
    "shape = (B, 3, img_size, img_size)\n",
    "    \n",
    "mean_keep, p95_keep, _ = measure_latency_ms(full_model, shape, device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim_model, shape, device=DEVICE)\n",
    "\n",
    "print(f\"Keep-all: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms | Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms | \\n\"\n",
    "      f\"Speedup={(mean_keep-mean_slim)/max(1e-6,mean_keep)*100:.2f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df7fd7e",
   "metadata": {},
   "source": [
    "## Download pre-trained gates for ViT on RTX4090\n",
    "\n",
    "To export your custom pruned model, obtain the gates from HawAda repo to know which layers to prune and how"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1c7e50-6163-40fb-86e6-df700bc9719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gated_model_repo = \"hawada/vit-base-patch16-224-rtx4090-gated\"\n",
    "\n",
    "ckpt_path  = hf_hub_download(gated_model_repo, \"pytorch_model.bin\")\n",
    "state_dict = torch.load(ckpt_path, map_location=DEVICE, weights_only=False)\n",
    "\n",
    "missing, unexpected = gated_model.load_state_dict(state_dict, strict=False)\n",
    "print(\"missing:\", len(missing), \"unexpected:\", len(unexpected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc242a-c47a-4369-8766-dd3576f911d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check configuration for pruning and export\n",
    "print(\"Policy for the probes during training:\", pack[\"probe_policy\"])\n",
    "print(\"\\nPolicy for the final pruning:\", pack[\"export_policy\"])\n",
    "\n",
    "slim_model = ViTAdapter.export_pruned(gated_model, \n",
    "                                   policy=pack[\"export_policy\"], \n",
    "                                   step=9999).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af590e6-8828-4f47-b96d-32a4d076e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search for the best export parameters\n",
    "\n",
    "num_heads = int(gated_model.config.num_attention_heads)\n",
    "grid_cfg = pack[\"recipe\"].get(\"export\").get(\"search\")\n",
    "head_grid = tuple(grid_cfg.get(\"grid_multiple_groups\"))\n",
    "ffn_grid  = tuple(grid_cfg.get(\"ffn_snaps\"))       \n",
    "\n",
    "search = vit_search_best_export(\n",
    "    gated_model.to(DEVICE),\n",
    "    export_fn=ViTAdapter.export_pruned,\n",
    "    num_heads=num_heads,\n",
    "    step=9999,  # no warmup\n",
    "    batch_shape=(B, 3, img_size, img_size),\n",
    "    device=pack[\"device\"],\n",
    "    make_policy=make_vit_policy,\n",
    "    grid=ViTGrid(head_multiple_grid=head_grid, ffn_snap_grid=ffn_grid),\n",
    ")\n",
    "\n",
    "\n",
    "slim_model = search.best_model.to(DEVICE)\n",
    "print(\"Best export params:\", search.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a41a655",
   "metadata": {},
   "source": [
    "### Measure latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8880c938-aff8-4abd-96a4-715702676d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting benchmarking with batch size = {B}...\")\n",
    "    \n",
    "full_model = ViTAdapter.export_keepall(gated_model).to(DEVICE)\n",
    "shape = (B, 3, img_size, img_size)\n",
    "    \n",
    "mean_keep, p95_keep, _ = measure_latency_ms(full_model, shape, device=DEVICE)\n",
    "mean_slim, p95_slim, _ = measure_latency_ms(slim_model, shape, device=DEVICE)\n",
    "\n",
    "print(f\"Keep-all: mean={mean_keep:.3f}ms p95={p95_keep:.3f}ms | Slim: mean={mean_slim:.3f}ms p95={p95_slim:.3f}ms | \\n\"\n",
    "      f\"Speedup={(mean_keep-mean_slim)/max(1e-6,mean_keep)*100:.2f}%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddba90f",
   "metadata": {},
   "source": [
    "# Distillation\n",
    "\n",
    "This slim model needs fine-tuning for your downstream task. In this notebook, we use 10-class classification head for ViT and ImageNet dataset.\n",
    "\n",
    "[!] For this step you can use any other device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6311f-8974-4237-ac32-53c90bd4008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.finetune import FinetuneConfig, finetune_student\n",
    "from core.distill import KDConfig\n",
    "\n",
    "teacher = pack[\"teacher\"].to(DEVICE) # Another instance of ViT for optional training / fine tuning\n",
    "\n",
    "ft_epochs = 1 # int(pack[\"recipe\"].get(\"finetune\").get(\"epochs\")\n",
    "print(f\"\\nStarting fine tuning for {ft_epochs} epochs...\")\n",
    "\n",
    "ft_cfg = FinetuneConfig(\n",
    "    epochs=ft_epochs,\n",
    "    lr=float(pack[\"recipe\"].get(\"finetune\").get(\"lr\")),\n",
    "    kd=KDConfig(**pack[\"recipe\"].get(\"trainer\").get(\"kd\")),\n",
    "    amp=bool(pack[\"recipe\"].get(\"trainer\").get(\"amp\")),\n",
    "    device=pack[\"device\"],\n",
    "    log_every=200,\n",
    ")\n",
    "\n",
    "slim = finetune_student(\n",
    "    slim_model,\n",
    "    teacher,\n",
    "    pack[\"train_loader\"],\n",
    "    get_student_logits=pack[\"get_s\"],\n",
    "    get_teacher_logits=pack[\"get_t\"],\n",
    "    cfg=ft_cfg,\n",
    "    val_loader=pack[\"val_loader\"],\n",
    "    save_best=True\n",
    ")\n",
    "\n",
    "\n",
    "# torch.save(slim, os.path.join(args.outdir, \"vit_slim_finetune.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41982895",
   "metadata": {},
   "source": [
    "Now you your own ViT version, optimized for RTX4090 and fine-tuned on ImageNet.\n",
    "\n",
    "HawAda framework allows you to optimize the model for other GPUs; To do it, follow the following steps:\n",
    "\n",
    "* Create your own recipe\n",
    "* Attach HawAda adapter to the model\n",
    "* Run the gates training on your device (see ResNet notebook)\n",
    "* Export pruned model after gates are trained\n",
    "* Run grid search to choose the best shapes\n",
    "* Run distillation (fine tuning) for your downstream task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hawada)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
